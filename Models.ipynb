{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from datetime import datetime\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import plotly.express as px\n",
    "# from mpl_toolkits import mplot3d\n",
    "\n",
    "import sklearn\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import itertools\n",
    "from itertools import product\n",
    "import statsmodels as sm\n",
    "import statsmodels.api as sm_api\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "import matplotlib\n",
    "from pylab import rcParams\n",
    "from tqdm import tqdm_notebook\n",
    "import fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2-9_Jan_2019.csv')\n",
    "\n",
    "df_jan = pd.read_csv('all_usage_jan_2019.csv')\n",
    "df_feb = pd.read_csv('all_usage_feb_2019.csv')\n",
    "df_mar = pd.read_csv('all_usage_mar_2019.csv')\n",
    "df_apr = pd.read_csv('all_usage_apr_2019.csv')\n",
    "df_may = pd.read_csv('all_usage_may_2019.csv')\n",
    "df_jun = pd.read_csv('all_usage_jun_2019.csv')\n",
    "df_jul = pd.read_csv('all_usage_jul_2019.csv')\n",
    "df_aug = pd.read_csv('all_usage_aug_2019.csv')\n",
    "df_sep = pd.read_csv('all_usage_sep_2019.csv')\n",
    "\n",
    "df_all = pd.read_csv('all_usage_Jan_to_Sep_new.csv')\n",
    "df_merged = pd.read_csv('usage_and_failed_search_new.csv')\n",
    "df_merged_no_hol = pd.read_csv('usage_and_failed_search_no_hol.csv')\n",
    "df_open = pd.read_csv('all_usage_open_new.csv')\n",
    "df_open_merged = pd.read_csv('usage_and_failed_open_sem.csv')\n",
    "df_close = pd.read_csv('all_usage_close_new.csv')\n",
    "df_close_merged = pd.read_csv('usage_and_failed_close_sem.csv')\n",
    "\n",
    "df_merged_no_hol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = ['7/1/2019', '14/1/2019', '21/1/2019', '28/1/2019', '4/2/2019', '11/2/2019', '18/2/2019', '25/2/2019', '4/3/2019', '11/3/2019', '18/3/2019', '25/3/2019', '1/4/2019', '22/4/2019', '29/4/2019', '13/5/2019', '27/5/2019', '10/6/2019', '17/6/2019', '24/6/2019', '1/7/2019', '8/7/2019', '15/7/2019', '22/7/2019', '5/8/2019', '12/8/2019', '19/8/2019', '26/8/2019', '2/9/2019', '9/9/2019', '16/9/2019', '23/9/2019', '30/9/2019']\n",
    "tue = ['8/1/2019', '15/1/2019', '22/1/2019', '29/1/2019', '5/2/2019', '12/2/2019', '26/2/2019', '5/3/2019', '12/3/2019', '19/3/2019', '26/3/2019', '2/4/2019', '9/4/2019', '23/4/2019', '30/4/2019', '7/5/2019', '14/5/2019', '21/5/2019', '28/5/2019', '4/6/2019', '11/6/2019', '18/6/2019', '25/6/2019', '2/7/2019', '9/7/2019', '23/7/2019', '30/7/2019', '6/8/2019', '13/8/2019', '20/8/2019', '27/8/2019', '3/9/2019', '10/9/2019', '17/9/2019', '24/9/2019']\n",
    "wed = ['2/1/2019', '9/1/2019', '16/1/2019', '23/1/2019', '30/1/2019', '6/2/2019', '13/2/2019', '20/2/2019', '27/2/2019', '6/3/2019', '13/3/2019', '20/3/2019', '27/3/2019', '3/4/2019', '10/4/2019', '17/4/2019', '24/4/2019', '1/5/2019', '8/5/2019', '15/5/2019', '22/5/2019', '29/5/2019', '5/6/2019', '12/6/2019', '19/6/2019', '26/6/2019', '3/7/2019', '10/7/2019', '24/7/2019', '31/7/2019', '7/8/2019', '14/8/2019', '21/8/2019', '28/8/2019', '4/9/2019', '11/9/2019', '18/9/2019', '25/9/2019']\n",
    "thu = ['3/1/2019', '10/1/2019', '17/1/2019', '24/1/2019', '31/1/2019', '7/2/2019', '14/2/2019', '21/2/2019', '28/2/2019', '7/3/2019', '14/3/2019', '21/3/2019', '28/3/2019', '4/4/2019', '11/4/2019', '18/4/2019', '25/4/2019', '2/5/2019', '16/5/2019', '23/5/2019', '30/5/2019', '6/6/2019', '13/6/2019', '20/6/2019', '27/6/2019', '4/7/2019', '11/7/2019', '18/7/2019', '25/7/2019', '1/8/2019', '8/8/2019', '15/8/2019', '22/8/2019', '29/8/2019', '5/9/2019', '12/9/2019', '19/9/2019', '26/9/2019']\n",
    "fri = ['4/1/2019', '11/1/2019', '18/1/2019', '25/1/2019', '1/2/2019', '8/2/2019', '15/2/2019', '22/2/2019', '1/3/2019', '8/3/2019', '15/3/2019', '22/3/2019', '29/3/2019', '5/4/2019', '19/4/2019', '26/4/2019', '3/5/2019', '10/5/2019', '17/5/2019', '24/5/2019', '31/5/2019', '7/6/2019', '14/6/2019', '21/6/2019', '28/6/2019', '5/7/2019', '12/7/2019', '19/7/2019', '26/7/2019', '2/8/2019', '9/8/2019', '16/8/2019', '23/8/2019', '30/8/2019', '6/9/2019', '13/9/2019', '20/9/2019', '27/9/2019']\n",
    "\n",
    "# day = []\n",
    "# for slot in df_all['Date']:\n",
    "#     if slot in mon:\n",
    "#         day.append('mon')\n",
    "#     if slot in tue:\n",
    "#         day.append('tue')\n",
    "#     if slot in wed:\n",
    "#         day.append('wed')\n",
    "#     if slot in thu:\n",
    "#         day.append('thu')\n",
    "#     if slot in fri:\n",
    "#         day.append('fri')\n",
    "# df_all['day'] = day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mon.extend(tue)\n",
    "mon.extend(wed)\n",
    "mon.extend(thu)\n",
    "mon.extend(fri)\n",
    "list(set((list(dict.fromkeys(df_merged['Date']))))-set(mon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_merged = ['7/1/2019', '14/1/2019', '21/1/2019', '28/1/2019', '4/2/2019', '11/2/2019', '18/2/2019', '25/2/2019', '4/3/2019', '11/3/2019', '18/3/2019', '25/3/2019', '1/4/2019', '8/4/2019', '15/4/2019', '22/4/2019', '29/4/2019', '6/5/2019', '13/5/2019', '20/5/2019', '27/5/2019', '10/6/2019', '17/6/2019', '24/6/2019', '1/7/2019', '8/7/2019', '15/7/2019', '22/7/2019', '5/8/2019', '12/8/2019', '19/8/2019', '26/8/2019', '2/9/2019', '9/9/2019', '16/9/2019', '23/9/2019', '30/9/2019']\n",
    "tue_merged = ['1/1/2019', '8/1/2019', '15/1/2019', '22/1/2019', '29/1/2019', '5/2/2019', '12/2/2019', '26/2/2019', '5/3/2019', '12/3/2019', '19/3/2019', '26/3/2019', '2/4/2019', '9/4/2019', '16/4/2019', '23/4/2019', '30/4/2019', '7/5/2019', '14/5/2019', '21/5/2019', '28/5/2019', '4/6/2019', '11/6/2019', '18/6/2019', '25/6/2019', '2/7/2019', '9/7/2019', '23/7/2019', '30/7/2019', '6/8/2019', '13/8/2019', '20/8/2019', '27/8/2019', '3/9/2019', '10/9/2019', '17/9/2019', '24/9/2019']\n",
    "wed_merged = ['2/1/2019', '9/1/2019', '16/1/2019', '23/1/2019', '30/1/2019', '6/2/2019', '13/2/2019', '20/2/2019', '27/2/2019', '6/3/2019', '13/3/2019', '20/3/2019', '27/3/2019', '3/4/2019', '10/4/2019', '17/4/2019', '24/4/2019', '1/5/2019', '8/5/2019', '15/5/2019', '22/5/2019', '29/5/2019', '5/6/2019', '12/6/2019', '19/6/2019', '26/6/2019', '3/7/2019', '10/7/2019', '24/7/2019', '31/7/2019', '7/8/2019', '14/8/2019', '21/8/2019', '28/8/2019', '4/9/2019', '11/9/2019', '18/9/2019', '25/9/2019']\n",
    "thu_merged = ['3/1/2019', '10/1/2019', '17/1/2019', '24/1/2019', '31/1/2019', '7/2/2019', '14/2/2019', '21/2/2019', '28/2/2019', '7/3/2019', '14/3/2019', '21/3/2019', '28/3/2019', '4/4/2019', '11/4/2019', '18/4/2019', '25/4/2019', '2/5/2019', '9/5/2019', '16/5/2019', '23/5/2019', '30/5/2019', '6/6/2019', '13/6/2019', '20/6/2019', '27/6/2019', '4/7/2019', '11/7/2019', '18/7/2019', '25/7/2019', '1/8/2019', '8/8/2019', '15/8/2019', '22/8/2019', '29/8/2019', '5/9/2019', '12/9/2019', '19/9/2019', '26/9/2019']\n",
    "fri_merged = ['4/1/2019', '11/1/2019', '18/1/2019', '25/1/2019', '1/2/2019', '8/2/2019', '15/2/2019', '22/2/2019', '1/3/2019', '8/3/2019', '15/3/2019', '22/3/2019', '29/3/2019', '5/4/2019', '12/4/2019', '19/4/2019', '26/4/2019', '3/5/2019', '10/5/2019', '17/5/2019', '24/5/2019', '31/5/2019', '7/6/2019', '14/6/2019', '21/6/2019', '28/6/2019', '5/7/2019', '12/7/2019', '19/7/2019', '26/7/2019', '2/8/2019', '9/8/2019', '16/8/2019', '23/8/2019', '30/8/2019', '6/9/2019', '13/9/2019', '20/9/2019', '27/9/2019']\n",
    "sat_merged = ['5/1/2019', '22/6/2019']\n",
    "\n",
    "day = []\n",
    "for slot in df_merged['Date']:\n",
    "    if slot in mon_merged:\n",
    "        day.append('1 - mon')\n",
    "    if slot in tue_merged:\n",
    "        day.append('2 - tue')\n",
    "    if slot in wed_merged:\n",
    "        day.append('3 - wed')\n",
    "    if slot in thu_merged:\n",
    "        day.append('4 - thu')\n",
    "    if slot in fri_merged:\n",
    "        day.append('5 - fri')\n",
    "    if slot in sat_merged:\n",
    "        day.append('6 - sat')\n",
    "df_merged['Day'] = day\n",
    "\n",
    "month = []\n",
    "for slot in df_merged['Date']:\n",
    "    (d, m, y) = slot.split('/')\n",
    "    m = int(m)\n",
    "    if (m == 1):\n",
    "        month.append('1 (Jan)')\n",
    "    elif (m == 2):\n",
    "        month.append('2 (Feb)')\n",
    "    elif (m == 3):\n",
    "        month.append('3 (Mar)')\n",
    "    elif (m == 4):\n",
    "        month.append('4 (Apr)')\n",
    "    elif (m == 5):\n",
    "        month.append('5 (May)')\n",
    "    elif (m == 6):\n",
    "        month.append('6 (Jun)')\n",
    "    elif (m == 7):\n",
    "        month.append('7 (Jul)')\n",
    "    elif (m == 8):\n",
    "        month.append('8 (Aug)')\n",
    "    elif (m == 9):\n",
    "        month.append('9 (Sep)')\n",
    "df_merged['Month'] = month\n",
    "\n",
    "demand = []\n",
    "demand = [1] * len(df_merged)\n",
    "df_merged['Demand'] = demand\n",
    "\n",
    "converted_regist = []\n",
    "for slot in df_merged['regist_dt_ICT']:\n",
    "    (h, m, s) = slot.split(':')\n",
    "    cell = int(h) + int(m)/60 + int(s)/3600\n",
    "    converted_regist.append(math.floor(cell))    \n",
    "df_merged['Converted_Regist'] = converted_regist\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart_station_int = []\n",
    "for station in df_merged['Station_depart.']:\n",
    "    depart_station_int.append(int(station[0:2]))\n",
    "df_merged['Station_depart_int'] = depart_station_int\n",
    "\n",
    "dest_station_int = []\n",
    "for station in df_merged['Station_dest.']:\n",
    "    dest_station_int.append(int(station[0:2]))\n",
    "df_merged['Station_dest_int'] = dest_station_int\n",
    "\n",
    "day_int = []\n",
    "for slot in df_merged['Date']:\n",
    "    if slot in mon_merged:\n",
    "        day_int.append(1)\n",
    "    if slot in tue_merged:\n",
    "        day_int.append(2)\n",
    "    if slot in wed_merged:\n",
    "        day_int.append(3)\n",
    "    if slot in thu_merged:\n",
    "        day_int.append(4)\n",
    "    if slot in fri_merged:\n",
    "        day_int.append(5)\n",
    "    if slot in sat_merged:\n",
    "        day_int.append(6)\n",
    "df_merged['Day_int'] = day_int\n",
    "\n",
    "month_int = []\n",
    "for slot in df_merged['Date']:\n",
    "    (d, m, y) = slot.split('/')\n",
    "    m = int(m)\n",
    "    if (m == 1):\n",
    "        month_int.append(1)\n",
    "    elif (m == 2):\n",
    "        month_int.append(2)\n",
    "    elif (m == 3):\n",
    "        month_int.append(3)\n",
    "    elif (m == 4):\n",
    "        month_int.append(4)\n",
    "    elif (m == 5):\n",
    "        month_int.append(5)\n",
    "    elif (m == 6):\n",
    "        month_int.append(6)\n",
    "    elif (m == 7):\n",
    "        month_int.append(7)\n",
    "    elif (m == 8):\n",
    "        month_int.append(8)\n",
    "    elif (m == 9):\n",
    "        month_int.append(9)\n",
    "df_merged['Month_int'] = month_int\n",
    "\n",
    "df_merged\n",
    "# converted_station_str = []\n",
    "# for station in df_merged['Station_depart.']:\n",
    "#     converted_station_str.append(station[0:2])\n",
    "\n",
    "# df_merged['Converted_Station_str'] = converted_station_str\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.scatter(df_merged['Converted_Station_str'], df_merged['Converted_Regist'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get the sum of demand IN EACH HOUR of EACH Station_depart. in a day\n",
    "# each_date = list(dict.fromkeys(df_merged['Date']))\n",
    "# each_station = list(dict.fromkeys(df_merged['Station_depart']))\n",
    "# each_hour\n",
    "# demand_at_depart = []\n",
    "# for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day = []\n",
    "for slot in df_open_merged['Date']:\n",
    "    if slot in mon_merged:\n",
    "        day.append('1 - mon')\n",
    "    if slot in tue_merged:\n",
    "        day.append('2 - tue')\n",
    "    if slot in wed_merged:\n",
    "        day.append('3 - wed')\n",
    "    if slot in thu_merged:\n",
    "        day.append('4 - thu')\n",
    "    if slot in fri_merged:\n",
    "        day.append('5 - fri')\n",
    "    if slot in sat_merged:\n",
    "        day.append('6 - sat')\n",
    "df_open_merged['Day'] = day\n",
    "\n",
    "month = []\n",
    "for slot in df_open_merged['Date']:\n",
    "    (d, m, y) = slot.split('/')\n",
    "    m = int(m)\n",
    "    if (m == 1):\n",
    "        month.append('1 (Jan)')\n",
    "    elif (m == 2):\n",
    "        month.append('2 (Feb)')\n",
    "    elif (m == 3):\n",
    "        month.append('3 (Mar)')\n",
    "    elif (m == 4):\n",
    "        month.append('4 (Apr)')\n",
    "    elif (m == 5):\n",
    "        month.append('5 (May)')\n",
    "    elif (m == 8):\n",
    "        month.append('8 (Aug)')\n",
    "    elif (m == 9):\n",
    "        month.append('9 (Sep)')\n",
    "df_open_merged['Month'] = month\n",
    "\n",
    "demand = []\n",
    "demand = [1] * len(df_open_merged)\n",
    "df_open_merged['Demand'] = demand \n",
    "\n",
    "df_open_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day = []\n",
    "for slot in df_close_merged['Date']:\n",
    "    if slot in mon_merged:\n",
    "        day.append('1 - mon')\n",
    "    if slot in tue_merged:\n",
    "        day.append('2 - tue')\n",
    "    if slot in wed_merged:\n",
    "        day.append('3 - wed')\n",
    "    if slot in thu_merged:\n",
    "        day.append('4 - thu')\n",
    "    if slot in fri_merged:\n",
    "        day.append('5 - fri')\n",
    "    if slot in sat_merged:\n",
    "        day.append('6 - sat')\n",
    "df_close_merged['Day'] = day\n",
    "\n",
    "month = []\n",
    "for slot in df_close_merged['Date']:\n",
    "    (d, m, y) = slot.split('/')\n",
    "    m = int(m)\n",
    "    if (m == 5):\n",
    "        month.append('5 (May)')\n",
    "    elif (m == 6):\n",
    "        month.append('6 (Jun)')\n",
    "    elif (m == 7):\n",
    "        month.append('7 (Jul)')\n",
    "    elif (m == 8):\n",
    "        month.append('8 (Aug)')\n",
    "df_close_merged['Month'] = month\n",
    "\n",
    "demand = []\n",
    "demand = [1] * len(df_close_merged)\n",
    "df_close_merged['Demand'] = demand \n",
    "\n",
    "df_close_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_merged['Station_depart.'], df_merged['Converted_Regist'])\n",
    "\n",
    "# df_merged_d = df_merged[df_merged['Date'] == '2/1/2019']\n",
    "# pd.crosstab(df_merged_d['Station_depart.'], df_merged_d['Converted_Regist'])\n",
    "\n",
    "# df_merged_dt = df_merged_d[df_merged_d['Converted_Regist'] == 7]\n",
    "# pd.crosstab(df_merged_dt['Station_depart.'], df_merged_dt['Converted_Regist'])\n",
    "\n",
    "# df_merged_s = df_merged[df_merged['Station_depart.'] == '01 Exit to Cham square']\n",
    "# pd.crosstab(df_merged_s['Station_depart.'], df_merged_s['Converted_Regist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#demand of each station each day in Jan\n",
    "df_merged_s = df_merged[df_merged['Station_depart.'] == '01 Exit to Cham square']\n",
    "# df_merged_sd = df_merged_s[df_merged_s['Station_depart.'] == '01 Exit to Cham square']\n",
    "\n",
    "df_merged_hm = df_merged_s.pivot_table(index='Day', columns='Converted_Regist', values='Demand', aggfunc=np.sum)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.set_context(context='notebook',font_scale=1.2)\n",
    "sns.heatmap(df_merged_hm, cmap=sns.color_palette(\"Blues\"), annot=True, fmt='.0f', linewidths = 1)\n",
    "pd.crosstab(df_merged_s['Day'], df_merged_s['Converted_Regist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['merged OD'] = df_merged['Station_depart.'] + df_merged['Station_dest.']\n",
    "print(len(df_merged['merged OD'].unique()))\n",
    "df_merged['merged OD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_s = df_merged[df_merged['Day'] == '1 - mon']\n",
    "df_merged_hm = df_merged_s.pivot_table(index='Station_depart.', columns='Converted_Regist', values='Demand', aggfunc=np.sum)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.set_context(context='notebook',font_scale=1.2)\n",
    "sns.heatmap(df_merged_hm, cmap=sns.color_palette(\"Blues\"), annot=True, fmt='.0f', linewidths = 1)\n",
    "# pd.crosstab(df_merged_s['Station_depart.'], df_merged_s['Converted_Regist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_s = df_merged[df_merged['Day'] == '2 - tue']\n",
    "\n",
    "df_merged_hm = df_merged_s.pivot_table(index='Station_depart.', columns='Converted_Regist', values='Demand', aggfunc=np.sum)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.set_context(context='notebook',font_scale=1.2)\n",
    "sns.heatmap(df_merged_hm, cmap=sns.color_palette(\"Blues\"), annot=True, fmt='.0f', linewidths = 1)\n",
    "# pd.crosstab(df_merged_s['Station_depart.'], df_merged_s['Converted_Regist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_s = df_merged[df_merged['Day'] == '3 - wed']\n",
    "\n",
    "df_merged_hm = df_merged_s.pivot_table(index='Station_depart.', columns='Converted_Regist', values='Demand', aggfunc=np.sum)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.set_context(context='notebook',font_scale=1.2)\n",
    "sns.heatmap(df_merged_hm, cmap=sns.color_palette(\"Blues\"), annot=True, fmt='.0f', linewidths = 1)\n",
    "# pd.crosstab(df_merged_s['Station_depart.'], df_merged_s['Converted_Regist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_s = df_merged[df_merged['Day'] == '4 - thu']\n",
    "\n",
    "df_merged_hm = df_merged_s.pivot_table(index='Station_depart.', columns='Converted_Regist', values='Demand', aggfunc=np.sum)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.set_context(context='notebook',font_scale=1.2)\n",
    "sns.heatmap(df_merged_hm, cmap=sns.color_palette(\"Blues\"), annot=True, fmt='.0f', linewidths = 1)\n",
    "# pd.crosstab(df_merged_s['Station_depart.'], df_merged_s['Converted_Regist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_s = df_merged[df_merged['Day'] == '5 - fri']\n",
    "\n",
    "df_merged_hm = df_merged_s.pivot_table(index='Station_depart.', columns='Converted_Regist', values='Demand', aggfunc=np.sum)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.set_context(context='notebook',font_scale=1.2)\n",
    "sns.heatmap(df_merged_hm, cmap=sns.color_palette(\"Blues\"), annot=True, fmt='.0f', linewidths = 1)\n",
    "# pd.crosstab(df_merged_s['Station_depart.'], df_merged_s['Converted_Regist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_s = df_merged[df_merged['Day'] == '6 - sat']\n",
    "\n",
    "df_merged_hm = df_merged_s.pivot_table(index='Station_depart.', columns='Converted_Regist', values='Demand', aggfunc=np.sum)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.set_context(context='notebook',font_scale=1.2)\n",
    "sns.heatmap(df_merged_hm, cmap=sns.color_palette(\"Blues\"), annot=True, fmt='.0f', linewidths = 1)\n",
    "# pd.crosstab(df_merged_s['Station_depart.'], df_merged_s['Converted_Regist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_merged_dm = df_merged.pivot_table(index='Month', columns='Converted_Regist', values='Demand', aggfunc=np.sum)\n",
    "plt.figure(figsize=(25,10))\n",
    "sns.set_context(context='notebook',font_scale=1.2)\n",
    "sns.heatmap(df_merged_dm, cmap=sns.color_palette(\"Blues\"), annot=True, fmt='.0f', linewidths = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmean = KMeans(n_clusters=3)\n",
    "X = df_merged[['Converted_Station', 'Converted_Regist', 'Usage\\'']]\n",
    "Kmean.fit(X)\n",
    "Kmean.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.axes(projection='3d')\n",
    "\n",
    "# # Data for a three-dimensional line\n",
    "# zline = df_merged['Demand']\n",
    "# xline = df_merged['Converted_Station_int']\n",
    "# yline = df_merged['Converted_Regist']\n",
    "# ax.plot3D(xline, yline, zline, 'gray')\n",
    "\n",
    "# # Data for three-dimensional scattered points\n",
    "# zdata = df_merged['Demand']\n",
    "# xdata = df_merged['Converted_Station_int']\n",
    "# ydata = df_merged['Converted_Regist']\n",
    "# ax.contour3D(xdata, ydata, zdata, c=zdata, cmap='Blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_merged[['Station_depart_int', 'Station_dest_int', 'Converted_Regist', 'Day_int', 'Month_int']]\n",
    "y = df_merged[['Demand']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.arange(min(X), max(X), 0.01)\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "# plt.scatter(X, y, color = 'red')\n",
    "# plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['Station_depart_int', 'Station_dest_int', 'Converted_Regist', 'Day_int', 'Month_int', 'Demand']\n",
    "df_use = df_merged[col]\n",
    "df_minmax = pd.DataFrame(MinMaxScaler().fit_transform(df_use), columns=df_use.columns)\n",
    "# df_standard = pd.DataFrame(StandardScaler().fit_transform(df), columns=df.columns)\n",
    "y = df_minmax[['Demand']]\n",
    "# y2 = df_standard[['num_critic_for_reviews']]\n",
    "df_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_minmax = df_minmax.corr()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Correlation of Minmax Normalization')\n",
    "sns.heatmap(corr_minmax, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_minmax[['Station_depart_int', 'Station_dest_int', 'Converted_Regist', 'Day_int', 'Month_int']]\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model_1 = LinearRegression()\n",
    "regression_model_1.fit(X1_train, y1_train)\n",
    "y_predicted_1 = regression_model_1.predict(X1_test)\n",
    "\n",
    "rmse = mean_squared_error(y1_test, y_predicted_1)\n",
    "mae = mean_absolute_error(y1_test, y_predicted_1)\n",
    "r2 = r2_score(y1_test, y_predicted_1)\n",
    "plt.scatter(y1_test, y_predicted_1)\n",
    "\n",
    "print('Slope:' ,regression_model_1.coef_)\n",
    "print('Intercept:', regression_model_1.intercept_)\n",
    "print('Root mean squared error: ', rmse)\n",
    "print('Mean absolute error: ', mae)\n",
    "print('R2 score: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_ts = []\n",
    "for slot in df_merged_no_hol['Date']:\n",
    "    (d, m, y) = slot.split('/')\n",
    "    m = int(m)\n",
    "    if (m == 1):\n",
    "        month_ts.append('Jan')\n",
    "    elif (m == 2):\n",
    "        month_ts.append('Feb')\n",
    "    elif (m == 3):\n",
    "        month_ts.append('Mar')\n",
    "    elif (m == 4):\n",
    "        month_ts.append('Apr')\n",
    "    elif (m == 5):\n",
    "        month_ts.append('May')\n",
    "    elif (m == 6):\n",
    "        month_ts.append('Jun')\n",
    "    elif (m == 7):\n",
    "        month_ts.append('July')\n",
    "    elif (m == 8):\n",
    "        month_ts.append('Aug')\n",
    "    elif (m == 9):\n",
    "        month_ts.append('Sep')\n",
    "df_merged_no_hol['Month'] = month_ts\n",
    "df_merged_no_hol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ARIMA\n",
    "df_o = df_merged_no_hol[df_merged_no_hol['Station_depart.'] == '12 CU Terrace']\n",
    "df_od = df_o[df_o['Station_dest.'] == '04 Engineering']\n",
    "df_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_date_demand = []\n",
    "each_date = list(dict.fromkeys(df_od['Date']))\n",
    "all_date = df_od.loc[ : , 'Date' ]\n",
    "\n",
    "# from datetime import date, timedelta\n",
    "# d = pd.to_datetime(each_date, dayfirst=True)\n",
    "# date_set = set(d[0] + timedelta(x) for x in range((d[-1] - d[0]).days))\n",
    "# missing = list(sorted(date_set - set(d)))\n",
    "# missing = str(pd.to_datetime(missing, dayfirst=True))\n",
    "# missing\n",
    "# for each in missing:\n",
    "#     each_date.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in each_date:\n",
    "    demand = 0\n",
    "    for each in all_date:\n",
    "        if date == each:\n",
    "            demand += 1\n",
    "    sum_date_demand.append(demand)\n",
    "    \n",
    "data = {'Date': each_date, 'Demand': sum_date_demand}    \n",
    "df_od_new = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od_new['Date']=pd.to_datetime(df_od_new['Date'], dayfirst=True)\n",
    "df_od_new.set_index('Date', inplace=True)\n",
    "df_od_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od_main2 = df_od_main\n",
    "df_od_main2['Timestamp']=pd.to_datetime(df_od_main['Timestamp'], dayfirst=True)\n",
    "df_od_main2.set_index('Timestamp', inplace=True)\n",
    "df_od_main2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts = df_od_new['Demand']\n",
    "ts = df_od_main['Demand']\n",
    "ts_train = ts[:'2019-04-30']\n",
    "ts_train2 = ts[:'2019-05-23']\n",
    "ts_test1 = ts['2019-05-01':'2019-05-23']\n",
    "ts_test2 = ts['2019-08-13':]\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.plot(ts_train)\n",
    "# plt.plot(ts_test1)\n",
    "# plt.plot(ts_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=15).mean()\n",
    "    rolstd = timeseries.rolling(window=15).std()\n",
    "    #Plot rolling statistics:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "\n",
    "test_stationarity(ts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log = np.log(ts_train)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ts_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg = ts_log.rolling(5).mean()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ts_log)\n",
    "plt.plot(moving_avg, color='red')\n",
    "\n",
    "# moving_avg = ts.rolling(9).mean()\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.plot(ts)\n",
    "# plt.plot(moving_avg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_no_mavg = ts_log - moving_avg\n",
    "ts_log_no_mavg\n",
    "\n",
    "# ts_no_mavg = ts - moving_avg\n",
    "# ts_no_mavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_no_mavg.dropna(inplace=True)\n",
    "ts_log_no_mavg\n",
    "\n",
    "# ts_no_mavg.dropna(inplace=True)\n",
    "# ts_no_mavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(ts_log_no_mavg)\n",
    "# test_stationarity(ts_no_mavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expweighted_avg = ts_log.ewm(halflife=5).mean()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ts_log)\n",
    "plt.plot(expweighted_avg, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_ewma_diff = ts_log - expweighted_avg\n",
    "test_stationarity(ts_log_ewma_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_diff = ts_log - ts_log.shift()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ts_log_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_diff.dropna(inplace=True)\n",
    "test_stationarity(ts_log_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(ts_train, period=5)\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(411)\n",
    "plt.plot(ts_train, label='Original')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonality')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log_decompose = residual\n",
    "ts_log_decompose.dropna(inplace=True)\n",
    "test_stationarity(ts_log_decompose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_acf = acf(ts_train, nlags=20)\n",
    "lag_pacf = pacf(ts_train, nlags=20, method='ols')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_train)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_train)),linestyle='--',color='gray')\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_train)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_train)),linestyle='--',color='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there are equal size, pick the latter one\n",
    "plot_acf(ts_train, title='ACF')\n",
    "plot_pacf(ts_train, title='PACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_train, order=(1, 1, 0))\n",
    "results_AR = model.fit(disp=-1)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ts_train)\n",
    "plt.plot(results_AR.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f' %sum((results_AR.fittedvalues-ts_train)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_train, order=(0, 1, 1))\n",
    "results_MA = model.fit(disp=-1)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ts_train)\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f' %sum((results_MA.fittedvalues-ts_train)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_train, order=(1, 1, 1))\n",
    "results_ARIMA = model.fit(disp=-1)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ts_train)\n",
    "plt.plot(results_ARIMA.fittedvalues, color='red')\n",
    "plt.title('RSS: %.4f' %sum((results_ARIMA.fittedvalues-ts_train)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(results_MA.fittedvalues, copy=True)\n",
    "print(predictions_ARIMA_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "print(predictions_ARIMA_diff_cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(ts_train.iloc[0], index=ts_train.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\n",
    "print(predictions_ARIMA_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
    "predictions_ARIMA = predictions_ARIMA_log\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ts_train)\n",
    "plt.plot(predictions_ARIMA)\n",
    "plt.title('RMSE: %.4f' %np.sqrt(sum((predictions_ARIMA-ts_train)**2)/len(ts_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['text.color'] = 'k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 18, 8\n",
    "# ts_sarima = ts_train\n",
    "df_od_main2 = df_od_main\n",
    "df_od_main2['Date']=pd.to_datetime(df_od_main['Timestamp'], dayfirst=True)\n",
    "df_od_main2 = df_od_main2.drop(['Timestamp'], axis=1)\n",
    "df_od_main2.set_index('Date', inplace=True)\n",
    "df_od_main2\n",
    "ts_sarima = df_od_main['Demand']\n",
    "# ts_log_sarima = np.log(ts_sarima)\n",
    "decomposition = sm_api.tsa.seasonal_decompose(ts_sarima, model='add', period=70)\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 15) for x in list(itertools.product(p, d, q))]\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm_api.tsa.statespace.SARIMAX(ts_train, order=param, seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('ARIMA{}x{}5 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm_api.tsa.statespace.SARIMAX(ts_sarima,\n",
    "                                order=(0, 1, 1),\n",
    "                                seasonal_order=(1, 1, 1, 5),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "print(results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start=pd.to_datetime('2019-02-04'), dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "ax = ts_sarima['2019-01-07':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Demand Daily')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean\n",
    "y_truth = ts_sarima['2019-02-04':]\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('MSE {}'.format(round(mse, 2)))\n",
    "print('RMSE {}'.format(round(np.sqrt(mse), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime('2019-04-01'), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "ax = ts_sarima['2019-01-07':].plot(label='observed')\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "# ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2019-02-04'), ts_sarima.index[-1], alpha=.1, zorder=-1)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Demand')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecasted = pred_dynamic.predicted_mean\n",
    "y_truth = ts_sarima['2019-02-04':]\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('MSE {}'.format(round(mse, 2)))\n",
    "print('RMSE {}'.format(round(np.sqrt(mse), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_uc = results.get_forecast(steps=8)\n",
    "pred_ci = pred_uc.conf_int()\n",
    "ax = ts_sarima.plot(label='observed', figsize=(20, 15))\n",
    "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CO2 Levels')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_uc = results.get_forecast(steps=8)\n",
    "forecast = pred_uc.predicted_mean\n",
    "forecast.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true-y_pred)/y_true))*100\n",
    "def plot_mavg(series, window, plot_intervals=False, scale=1.96):\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.plot(rolling_mean, 'r')\n",
    "    \n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:]-rolling_mean[window:])\n",
    "        lower_bound = rolling_mean - (mae+scale*deviation)\n",
    "        upper_bound = rolling_mean + (mae+scale*deviation)\n",
    "        plt.plot(upper_bound, 'r--')\n",
    "        plt.plot(lower_bound, 'r--')\n",
    "    \n",
    "    plt.plot(series[window:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mavg(df_od_main, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mavg(ts_train, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mavg(ts_train, 20, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_smoothing(series, alpha):\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1-alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def plot_exp_smoothing(series, alphas):\n",
    "    plt.figure(figsize=(14,7))\n",
    "    for alpha in alphas:\n",
    "        plt.plot(exp_smoothing(series, alpha), label='Alpha {}'.format(alpha))\n",
    "    plt.plot(series.values, \"c\", label='Actual')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.axis('tight')\n",
    "\n",
    "plot_exp_smoothing(ts_train, [0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_plot(y, lags=None, figsize=(14,7), style='bmh'):\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('TSA-DF: p={0:5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "ts_plot(ts_train, lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_diff = ts_train-ts_train.shift()\n",
    "ts_diff.dropna(inplace=True)\n",
    "ts_plot(ts_diff, lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = range(0,3)\n",
    "d = 0\n",
    "qs = range(0,3)\n",
    "Ps = range(0,3)\n",
    "D = 1\n",
    "Qs = range(0,3)\n",
    "s = 15\n",
    "\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_SARIMA(parameters_list, d, D, s):\n",
    "    \n",
    "    results = []\n",
    "    best_aic = float('inf')\n",
    "    \n",
    "    for param in tqdm_notebook(parameters_list):\n",
    "        try: \n",
    "            model = sm_api.tsa.statespace.SARIMAX(ts_train, order=(param[0], d, param[1]), seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        aic = model.aic\n",
    "        \n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "        \n",
    "    result_table = pd.DataFrame(results)\n",
    "    result_table.columns = ['parameters', 'aic']\n",
    "    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_table = optimize_SARIMA(parameters_list, d, D, s)\n",
    "print(len(result_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, q, P, Q = result_table.parameters[0]\n",
    "best_model = sm_api.tsa.statespace.SARIMAX(ts_train, order=(p,d,q), seasonal_order=(P,D,Q,s)).fit(disp=-1)\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(best_model.predict(start=ts_train.shape[0], end=ts_train.shape[0]+14))\n",
    "print(mean_absolute_percentage_error(ts_test1[s+d], best_model.fittedvalues[s+d]))\n",
    "# plt.plot(ts_test1)\n",
    "# plt.plot(best_model.fittedvalues[s+d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROPHET\n",
    "df_o = df_merged_no_hol[df_merged_no_hol['Station_depart.'] == '12 CU Terrace']\n",
    "df_od = df_o[df_o['Station_dest.'] == '04 Engineering']\n",
    "df_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od['DateTime'] = df_od[df_od.columns[0:2]].apply(lambda x : '/' .join(x.astype(str)),axis=1)\n",
    "df_od['DateTime'] = df_od['DateTime'].str.replace(\":\", \"/\")\n",
    "timestamp_converted = []\n",
    "for slot in df_od['DateTime']:\n",
    "    (d, m, y, hh, mm, ss) = slot.split('/')\n",
    "    d = int(d)\n",
    "    m = int(m)\n",
    "    y = int(y)\n",
    "    hh = int(hh)\n",
    "#     mm = int(mm)\n",
    "#     ss = int(ss)\n",
    "#     timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh, minute=mm, second=ss)\n",
    "    timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "    timestamp_converted.append(timestamp)\n",
    "df_od['Timestamp'] = timestamp_converted\n",
    "\n",
    "demand = []\n",
    "demand = [1] * len(df_od)\n",
    "df_od['Demand'] = demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od['Date'] = pd.to_datetime(df_od['Date'], dayfirst=True)\n",
    "df_od.set_index('Date', inplace=True)\n",
    "df_od = df_od[:'2019-05-23']\n",
    "df_od = df_od.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od_main = df_od.groupby('Timestamp').sum()\n",
    "df_od_main = df_od_main.reset_index()\n",
    "df_od_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = each_timestamp = list(dict.fromkeys(df_od['Date']))\n",
    "oper_time = list(range(7,22))\n",
    "timestamp_missing = []\n",
    "\n",
    "for each in date:\n",
    "    each = str(each)\n",
    "    each = each[0:11]\n",
    "    for h in oper_time:\n",
    "        (y, m, d) = each.split('-')\n",
    "        d = int(d)\n",
    "        m = int(m)\n",
    "        y = int(y)\n",
    "        hh = h\n",
    "        timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "        timestamp_missing.append(timestamp)\n",
    "\n",
    "timestamp_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamp_fill = list(set(timestamp_missing) - set((list(dict.fromkeys(df_od_main['Timestamp'])))))\n",
    "# timestamp_missing\n",
    "\n",
    "demand_fill = []\n",
    "demand_fill = [0] * len(timestamp_fill)\n",
    "# demand_fill\n",
    "\n",
    "data_fill = {'Timestamp': timestamp_fill, 'Demand': demand_fill}    \n",
    "df_od_fill = pd.DataFrame(data_fill)\n",
    "df_od_fill = df_od_fill.sort_values('Timestamp')\n",
    "df_od_fill = df_od_fill.reset_index(drop=True)\n",
    "df_od_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od_main = df_od_main.append(df_od_fill)\n",
    "df_od_main = df_od_main.sort_values('Timestamp')\n",
    "df_od_main = df_od_main.reset_index(drop=True)\n",
    "df_od_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prophet = df_od_main[['Timestamp', 'Demand']]\n",
    "df_prophet = df_prophet.rename(columns={'Timestamp': 'ds', 'Demand': 'y'})\n",
    "df_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "# df_prophet['cap'] = 10\n",
    "# df_prophet['floor'] = 0\n",
    "# m = Prophet(growth='logistic', changepoint_prior_scale=0.5)\n",
    "m = Prophet(changepoint_prior_scale=0.2) \n",
    "m.fit(df_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=720, freq='H')\n",
    "# future['cap'] = 10\n",
    "# future['floor'] = 0\n",
    "future = future[(future['ds'].dt.hour >= 7) & (future['ds'].dt.hour <=21)]\n",
    "forecast = m.predict(future)\n",
    "yhat_round = []\n",
    "for each in forecast['yhat']:\n",
    "#     if (each < 0):\n",
    "#         new = 0\n",
    "#     elif (((each*10)%10) >= 5):\n",
    "#         new = math.ceil(each)\n",
    "#     else:\n",
    "    each = math.ceil(each)\n",
    "    yhat_round.append(new)\n",
    "# forecast['yhat'] = yhat_round\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_comparison_dataframe(historical, forecast):\n",
    "#     yhat_ceil = []\n",
    "#     for each in forecast['yhat']:\n",
    "#         new = math.floor(each)\n",
    "# #         print(new)\n",
    "#         yhat_ceil.append(new)\n",
    "#     forecast['yhat_ceil'] = yhat_ceil\n",
    "    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))\n",
    "\n",
    "cmp_df = make_comparison_dataframe(df_prophet, forecast)\n",
    "\n",
    "def cal_fc_err(df, prediction_size):\n",
    "    df = df.copy()\n",
    "    df['e'] = df['y'] - df['yhat']\n",
    "#     print(df['e'])\n",
    "    df['p'] = 100 * df['e'] / df['y']\n",
    "    print(df['p'])\n",
    "    predicted_part = df[-prediction_size:]\n",
    "    \n",
    "    err_mean = lambda error_name: np.mean(np.abs(predicted_part[error_name]))\n",
    "    \n",
    "    return {'MAPE': err_mean('p'), 'MAE': err_mean('e')}\n",
    "\n",
    "for err_name, err_value in cal_fc_err(cmp_df, 720).items():\n",
    "    print(err_name, err_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation\n",
    "df_cv = cross_validation(m,horizon = '1 days')\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import performance_metrics\n",
    "df_p = performance_metrics(df_cv)\n",
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_prophet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
