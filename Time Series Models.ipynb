{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "from pylab import rcParams\n",
    "from tqdm import tqdm_notebook\n",
    "import fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged_no_hol_old = pd.read_csv('usage_and_failed_search_no_hol.csv')\n",
    "df_merged_no_hol = pd.read_csv('no_hol_v2_edited.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = df_merged_no_hol[df_merged_no_hol['Station_depart.'] == '12 CU Terrace']\n",
    "df_od = df_o[df_o['Station_dest.'] == '04 Engineering']\n",
    "\n",
    "df_od.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od['DateTime'] = df_od[df_od.columns[0:2]].apply(lambda x : '/' .join(x.astype(str)),axis=1)\n",
    "df_od['DateTime'] = df_od['DateTime'].str.replace(\":\", \"/\")\n",
    "\n",
    "demand = []\n",
    "demand = [1] * len(df_od)\n",
    "df_od['Demand'] = demand\n",
    "df_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od['Date'] = pd.to_datetime(df_od['Date'], dayfirst=True)\n",
    "df_od.set_index('Date', inplace=True)\n",
    "# df_train = df_train_clone = df_od[:'2019-04-30']\n",
    "# df_test = df_test_clone = df_od['2019-05-01':'2019-05-23']\n",
    "df_train = df_train_clone = df_od[:'2019-03-29']\n",
    "df_test = df_test_clone = df_od['2019-04-01':'2019-04-30']\n",
    "df_train2 = df_train_clone2 = df_od['2019-08-13':'2019-09-20']\n",
    "df_test2 = df_test_clone2 = df_od['2019-09-23':'2019-09-30']\n",
    "# df_cv = df_cv_clone = df_od[:'2019-05-23']\n",
    "df_train_clone = df_train_clone.reset_index()\n",
    "df_test_clone = df_test_clone.reset_index()\n",
    "df_train_clone2 = df_train_clone2.reset_index()\n",
    "df_test_clone2 = df_test_clone2.reset_index()\n",
    "# df_cv_clone = df_cv_clone.reset_index()\n",
    "df_train_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(dict.fromkeys(df_test_clone['Date']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 7\n",
    "stop = 21.5\n",
    "days_forecast = 18\n",
    "#14*15+45 or 18*15+60 (train = :2019-03-29) or  for 2018/2, 6 for 2019/1\n",
    "hours_multiplier = 15\n",
    "prediction_size = days_forecast*hours_multiplier+60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od = df_od.reset_index()\n",
    "df_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df, df_main, type):\n",
    "    date = list(dict.fromkeys(df_main['Date']))\n",
    "    if (type == True):\n",
    "        oper_time = list(np.arange(start, stop, 0.5))\n",
    "    else:\n",
    "        oper_time = list(np.arange(start, stop))\n",
    "    timestamp_all = []\n",
    "\n",
    "    for each in date:\n",
    "        each = str(each)\n",
    "        each = each[0:11]\n",
    "        for h in oper_time:\n",
    "            (y, m, d) = each.split('-')\n",
    "            d, m, y, hh = int(d), int(m), int(y), int(h)\n",
    "            if (((h*10)%10) == 5):\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh, minute=30)\n",
    "            else:\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "            timestamp_all.append(timestamp)\n",
    "\n",
    "    timestamp_fill = list(set(timestamp_all) - set((list(dict.fromkeys(df_main['Timestamp'])))))\n",
    "    \n",
    "    demand_fill = []\n",
    "    demand_fill = [0] * len(timestamp_fill)\n",
    "\n",
    "    data_fill = {'Timestamp': timestamp_fill, 'Demand': demand_fill}    \n",
    "    df_od_fill = pd.DataFrame(data_fill)\n",
    "    df_od_fill = df_od_fill.sort_values('Timestamp')\n",
    "    df_od_fill = df_od_fill.reset_index(drop=True)\n",
    "    \n",
    "    df, backup = group_by_time(df)\n",
    "    \n",
    "    df = df.append(df_od_fill)\n",
    "    df = df.sort_values('Timestamp')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def group_by_time(df):\n",
    "    df = df.groupby('Timestamp').sum()\n",
    "    backup = df\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df, backup\n",
    "\n",
    "def create_window(df, type):\n",
    "    converted_regist = []\n",
    "\n",
    "    for slot in df['regist_dt_ICT']:\n",
    "        (h, m, s) = slot.split(':')\n",
    "        if (type == True):\n",
    "            if (int(m) >= 30):\n",
    "                time = int(h) + 0.5\n",
    "            else:\n",
    "                time = int(h)\n",
    "        else:\n",
    "            time = int(h)\n",
    "        converted_regist.append(time)    \n",
    "\n",
    "    df['Converted_Regist'] = converted_regist\n",
    "\n",
    "    df = df[df['Converted_Regist'] >= start]\n",
    "    df = df[df['Converted_Regist'] < stop]\n",
    "    df = df.drop(['Converted_Regist'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_timestamp(df, type): \n",
    "    timestamp_converted = []\n",
    "    for slot in df['DateTime']:\n",
    "        (d, m, y, hh, mm, ss) = slot.split('/')\n",
    "        d, m, y, hh, mm = int(d), int(m), int(y), int(hh), int(mm)\n",
    "        if (type == True):\n",
    "            if (mm >= 30):\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh, minute=30)\n",
    "            else:\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "        else:\n",
    "            timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "        timestamp_converted.append(timestamp)\n",
    "    df['Timestamp'] = timestamp_converted\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clone = create_timestamp(df_train_clone, True)\n",
    "train_framed = create_window(df_train_clone, True)\n",
    "train, backup_train = group_by_time(train_framed)\n",
    "train = train_clone = fill_missing(train, df_train_clone, True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_clone = create_timestamp(df_test_clone, False)\n",
    "test_framed = create_window(df_test_clone, False)\n",
    "test, backup_test = group_by_time(test_framed)\n",
    "test = test_clone = fill_missing(test, df_test_clone, False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clone2 = create_timestamp(df_train_clone2, True)\n",
    "train_framed2 = create_window(df_train_clone2, True)\n",
    "train2, backup_train2 = group_by_time(train_framed2)\n",
    "train2 = train_clone2 = fill_missing(train2, df_train_clone2, True)\n",
    "train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_clone2 = create_timestamp(df_test_clone2, False)\n",
    "test_framed2= create_window(df_test_clone2, False)\n",
    "test2, backup_test2 = group_by_time(test_framed2)\n",
    "test2 = test_clone2 = fill_missing(test2, df_test_clone2, False)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv, backup_cv = group_by_time(df_cv)\n",
    "# cv = fill_missing(cv, df_cv_clone)\n",
    "# cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prophet = cv[['Timestamp', 'Demand']]\n",
    "# df_prophet = df_prophet.rename(columns={'Timestamp': 'ds', 'Demand': 'y'})\n",
    "# df_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prophet = train[['Timestamp', 'Demand']]\n",
    "df_prophet = df_prophet.rename(columns={'Timestamp': 'ds', 'Demand': 'y'})\n",
    "df_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "df_prophet['cap'] = 5\n",
    "df_prophet['floor'] = 0\n",
    "m = Prophet(growth='logistic', changepoint_prior_scale=0.6)\n",
    "#best: yhat scale 0.6 round at 3.7, yhat_upper 0.8 round at 8\n",
    "# m = Prophet(changepoint_prior_scale=0.2)\n",
    "m.fit(df_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=800, freq='H')\n",
    "future['cap'] = 5\n",
    "future['floor'] = 0\n",
    "future = future[(future['ds'].dt.hour >= start) & (future['ds'].dt.hour < stop)]\n",
    "future = future[future['ds'].dt.dayofweek < 5]\n",
    "forecast = m.predict(future)\n",
    "yhat_round = []\n",
    "for each in forecast['yhat']:\n",
    "    if(each < 0):\n",
    "        each = 0\n",
    "    elif ((each*10)%10 >= 3.7):\n",
    "        each = math.ceil(each)\n",
    "    else:\n",
    "        each = math.floor(each)\n",
    "    yhat_round.append(each)\n",
    "# forecast['yhat_round'] = yhat_round\n",
    "# forecast[['ds', 'yhat_round', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "forecast['yhat_nr'] = forecast['yhat']\n",
    "forecast['yhat'] = yhat_round\n",
    "forecast[['ds', 'yhat', 'yhat_nr', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clone = train_clone.set_index('Timestamp')\n",
    "# test_clone = test_clone.set_index('Timestamp')\n",
    "# train_clone2 = train_clone2.set_index('Timestamp')\n",
    "# test_clone2 = test_clone2.set_index('Timestamp')\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# plt.subplot(211)\n",
    "# plt.plot(train_clone)\n",
    "# plt.plot(test_clone)\n",
    "# plt.subplot(212)\n",
    "# plt.plot(train_clone2)\n",
    "# plt.plot(test_clone2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_eva(train, forecast, prediction_size):\n",
    "    forecast = forecast[:prediction_size]\n",
    "    return forecast.set_index('ds')[['yhat', 'yhat_nr', 'yhat_lower', 'yhat_upper']].join(train.set_index('ds'))\n",
    "\n",
    "def cal_SMAPE(df):\n",
    "    smape = 100/len(df) * np.sum(2 * np.abs(df['yhat'] - df['y']) / (np.abs(df['y']) + np.abs(df['yhat'])))\n",
    "    print(df)\n",
    "    return 'sMAPE', smape, 'accuracy', 100-smape\n",
    "\n",
    "train = join_eva(df_prophet, forecast, len(df_prophet))\n",
    "\n",
    "print(cal_SMAPE(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_test(test, forecast, prediction_size):\n",
    "    df_test = test.rename(columns={'Timestamp': 'ds', 'Demand': 'y'})\n",
    "    forecast = forecast[len(df_prophet):len(df_prophet)+prediction_size]\n",
    "    return forecast.set_index('ds')[['yhat', 'yhat_nr', 'yhat_lower', 'yhat_upper']].join(df_test.set_index('ds'))\n",
    "\n",
    "eva_df = join_test(test, forecast, prediction_size)\n",
    "\n",
    "print(cal_SMAPE(eva_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eva_df.tail(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scales = np.arange(0.1,1.0,0.1)\n",
    "rounding_boundary = np.arange(1, 10, 1)\n",
    "\n",
    "def grid_search():\n",
    "    scale = [] \n",
    "    round_at = []\n",
    "    eva_acc = []\n",
    "    test_acc = []\n",
    "    for param in scales:\n",
    "        for boundary in rounding_boundary:\n",
    "            scale.append(param)\n",
    "            round_at.append(boundary)\n",
    "            df_prophet['cap'] = 5\n",
    "            df_prophet['floor'] = 0\n",
    "            m = Prophet(growth='logistic', changepoint_prior_scale=param)\n",
    "#             m = Prophet(changepoint_prior_scale=param) \n",
    "            m.fit(df_prophet)\n",
    "            future = m.make_future_dataframe(periods=768, freq='H')\n",
    "            future['cap'] = 5\n",
    "            future['floor'] = 0\n",
    "            future = future[(future['ds'].dt.hour >= start) & (future['ds'].dt.hour < stop)]\n",
    "            future = future[future['ds'].dt.dayofweek < 5]\n",
    "            forecast = m.predict(future)\n",
    "            yhat_round = []            \n",
    "            for each in forecast['yhat']:\n",
    "                if (each < 0):\n",
    "                    each = 0\n",
    "                elif (((each*10)%10) >= boundary):\n",
    "                    each = math.ceil(each)\n",
    "                else:\n",
    "                    each = math.floor(each)\n",
    "                yhat_round.append(each)\n",
    "            forecast['yhat_nr'] = forecast['yhat']\n",
    "            forecast['yhat'] = yhat_round\n",
    "\n",
    "            train = join_eva(df_prophet, forecast, len(df_prophet))\n",
    "            acc = cal_SMAPE(train)\n",
    "            eva_acc.append(acc)\n",
    "\n",
    "            eva_df = join_test(test2, forecast, prediction_size)\n",
    "            acc = cal_SMAPE(eva_df)\n",
    "            test_acc.append(acc)\n",
    "    \n",
    "    return scale, round_at, eva_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale, round_at, eva_acc, test_acc = grid_search()\n",
    "grid = {'Scale': scale, 'Round at': round_at, 'Fit': eva_acc, 'Test': test_acc}    \n",
    "df_grid = pd.DataFrame(grid)\n",
    "df_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid_sorted = df_grid.sort_values('Test')\n",
    "df_grid_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     n = training_series.shape[0]\n",
    "#     d = np.abs(  np.diff( training_series) ).sum()/(n-1)\n",
    "    \n",
    "#     errors = np.abs(testing_series - prediction_series )\n",
    "#     return errors.mean()/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_comparison_dataframe(historical, forecast):\n",
    "#     return forecast.set_index('ds')[['yhat', 'yhat_nr', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))\n",
    "\n",
    "# cmp_df = make_comparison_dataframe(df_prophet, forecast)\n",
    "\n",
    "# def cal_fc_err(df, prediction_size):\n",
    "#     df = df.copy()\n",
    "#     df['e'] = df['y'] - df['yhat']\n",
    "#     df['p'] = 100 * df['e'] / df['y']\n",
    "#     predicted_part = df[:prediction_size]\n",
    "    \n",
    "#     err_mean = lambda error_name: np.mean(np.abs(predicted_part[error_name]))\n",
    "    \n",
    "#     return {'MAPE': err_mean('p'), 'MAE': err_mean('e')}\n",
    "\n",
    "# for err_name, err_value in cal_fc_err(cmp_df, 217).items():\n",
    "#     print(err_name, err_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(211)\n",
    "plt.plot(joined['yhat'], label='predict')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.plot(cmp_df['yhat_lower'], label='lower bound')\n",
    "# plt.plot(cmp_df['yhat_upper'], label='upper bound')\n",
    "plt.subplot(212)\n",
    "plt.plot(joined['y'], label='actual')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation\n",
    "df_cv = cross_validation(m, initial = '90 days', horizon = '20 days')\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import performance_metrics\n",
    "df_p = performance_metrics(df_cv)\n",
    "df_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import product\n",
    "import statsmodels as sm\n",
    "import statsmodels.api as sm_api\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = list(dict.fromkeys(df_od['Date']))\n",
    "oper_time = list(range(7,22))\n",
    "timestamp_all = []\n",
    "\n",
    "for each in date:\n",
    "    each = str(each)\n",
    "    each = each[0:11]\n",
    "    for h in oper_time:\n",
    "        (y, m, d) = each.split('-')\n",
    "        d = int(d)\n",
    "        m = int(m)\n",
    "        y = int(y)\n",
    "        hh = h\n",
    "        timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "        timestamp_all.append(timestamp)\n",
    "\n",
    "timestamp_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arima = create_timestamp(df_train_clone, False)\n",
    "train_arima = fill_missing(train_arima, df_train_clone, False)\n",
    "train_arima = train_arima.drop(['Converted_Regist'], axis=1)\n",
    "train_arima = train_arima.set_index(['Timestamp'])\n",
    "train_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arima = create_timestamp(df_test_clone, False)\n",
    "test_arima = fill_missing(test_arima, df_test_clone, False)\n",
    "test_arima = test_arima.drop(['Converted_Regist'], axis=1)\n",
    "test_arima = test_arima.set_index(['Timestamp'])\n",
    "test_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df_arima['Demand']\n",
    "ts_train = ts[:'2019-04-30']\n",
    "ts_test = ts['2019-05-01':'2019-05-23']\n",
    "ts_train2 = ts[:'2019-05-23']\n",
    "ts_test2 = ts['2019-08-13':]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(ts_train, label='train')\n",
    "plt.plot(ts_test, label='test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(212)\n",
    "plt.plot(ts_train2, label='train2')\n",
    "plt.plot(ts_test2, label='test2')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=15).mean()\n",
    "    rolstd = timeseries.rolling(window=15).std()\n",
    "    #Plot rolling statistics:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "\n",
    "test_stationarity(train_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(train_arima, period=5)\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(411)\n",
    "plt.plot(train_arima, label='Original')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonality')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = range(0, 3)\n",
    "d = 0\n",
    "q = range(0, 3)\n",
    "s = 75\n",
    "pdq = []\n",
    "seasonal_pdq = []\n",
    "for ar in p:\n",
    "    for ma in q:\n",
    "        param = (ar, d, ma)\n",
    "        sparam = (ar, d, ma, s)\n",
    "        pdq.append(param)\n",
    "        seasonal_pdq.append(sparam)\n",
    "\n",
    "# pdq = list(itertools.product(p, d, q))\n",
    "# seasonal_pdq = [(x[0], x[1], x[2], 75) for x in list(itertools.product(p, q))]\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm_api.tsa.statespace.SARIMAX(train_arima, order=param, seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('SARIMAX{}x{}75 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm_api.tsa.statespace.SARIMAX(train_arima,\n",
    "                                order=(0, 0, 0),\n",
    "                                seasonal_order=(2, 0, 2, 75),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "print(results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start=pd.to_datetime('2019-04-01 07:00:00'), dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "\n",
    "ax = train_arima['2019-01-07 07:00:00':].plot(label='observed', figsize=(14, 7))\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Demand Hourly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean\n",
    "for each in y_forecasted:\n",
    "    if (each < 0):\n",
    "        y_forecasted = y_forecasted.replace(each, 0)\n",
    "    elif (((each*10)%10) > 6):\n",
    "        y_forecasted = y_forecasted.replace(each, math.ceil(each))\n",
    "    else:\n",
    "        y_forecasted = y_forecasted.replace(each, math.floor(each))\n",
    "y_truth = train_arima['2019-04-01 07:00:00':].Demand\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "smape = 100/len(y_truth) * np.sum(2 * np.abs(y_forecasted - y_truth) / (np.abs(y_forecasted) + np.abs(y_truth)))\n",
    "print('MSE {}'.format(round(mse, 2)))\n",
    "print('RMSE {}'.format(round(np.sqrt(mse), 2)))\n",
    "print('sMAPE {}'.format(smape, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime('2019-04-01 07:00:00'), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "ax = train_arima['2019-01-07 07:00:00':].plot(label='observed', figsize=(14, 7))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2019-04-01'), train_arima.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Demand Hourly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecasted = pred_dynamic.predicted_mean\n",
    "for each in y_forecasted:\n",
    "    if (each < 0):\n",
    "        y_forecasted = y_forecasted.replace(each, 0)\n",
    "    elif (((each*10)%10) > 6):\n",
    "        y_forecasted = y_forecasted.replace(each, math.ceil(each))\n",
    "    else:\n",
    "        y_forecasted = y_forecasted.replace(each, math.floor(each))\n",
    "y_truth = train_arima['2019-04-01 07:00:00':].Demand\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "smape = 100/len(y_truth) * np.sum(2 * np.abs(y_forecasted - y_truth) / (np.abs(y_forecasted) + np.abs(y_truth)))\n",
    "print('MSE {}'.format(round(mse, 2)))\n",
    "print('RMSE {}'.format(round(np.sqrt(mse), 2)))\n",
    "print('sMAPE {}'.format(smape, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_uc = results.get_forecast(steps=210)\n",
    "pred_ci = pred_uc.conf_int()\n",
    "\n",
    "plt.plot(pred_uc.predicted_mean)\n",
    "# ax = train_arima.plot(label='observed', figsize=(20, 15))\n",
    "# pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "# ax.fill_between(pred_ci.index,\n",
    "#                 pred_ci.iloc[:, 0],\n",
    "#                 pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "# ax.set_xlabel('Date')\n",
    "# ax.set_ylabel('Demand Hourly')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "y_forecasted = pred_uc.predicted_mean\n",
    "for each in y_forecasted:\n",
    "    if (each < 0):\n",
    "        each = 0\n",
    "    elif (((each*10)%10) > 5):\n",
    "        each = math.ceil(each)\n",
    "    else:\n",
    "        each = math.floor(each)\n",
    "    \n",
    "    new_list.append(each)\n",
    "\n",
    "test_arima['predict'] = new_list\n",
    "test_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_SMAPE(df):\n",
    "    smape = 100/len(df) * np.sum(2 * np.abs(df['Demand'] - df['predict']) / (np.abs(df['Demand']) + np.abs(df['predict'])))\n",
    "    print(df)\n",
    "    return 'sMAPE', smape, 'accuracy', 100-smape\n",
    "\n",
    "cal_SMAPE(test_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "adf, pvalue, usedlag_, nobs_, critical_values_, icbest_ = adfuller(ts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "fig, ax = plt.subplots(2,1)\n",
    "fig = sm.graphics.tsa.plot_acf(train, lags=50, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(train, lags=50, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(train, order=(0,1,1), \n",
    "                                  seasonal_order(0,1,1,7))\n",
    "                                 .fit(max_iter=50, method='powell')\n",
    "res = model.resid\n",
    "fig, ax = plt.subplots(2,1)\n",
    "fig = sm.graphics.tsa.plot_acf(res, lags=50, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(res, lags=50, ax=ax[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
