{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 routes - FBPROPHET\n",
    "\n",
    "|Depart - Destination Station|2018/2|scale & rounding|upper|2019/1|scale & rounding|upper|Remark\n",
    "|----------------------------|------|----------------|-----|------|----------------|-----|------\n",
    "|12 CU Terrace - 04 Engineering|72.4%|0.3 and 7|N|56.1%|0.1 and 9|N|logistics 0-4/0-6|\n",
    "|04 Engineering - 12 CU Terrace|73.0%|0.4 and 4|N|57.0%|0.5 and 4|N|logistic 0-2/0-3|\n",
    "|12 CU Terrace - 03 Sala Prakeaw|79.8%|0.8 and 9|N|80.0%|0.1 and 9|Y|logistics 0-3 both|\n",
    "|04 Engineering - 22 Siam Square Soi 8|84.0%|0.8 and 4|N|72.6%|0.9 and 6|N|logistics 0-4 both|\n",
    "|04 Engineering - 21 Wittayakit Building|83.0%|0.8 and 3|N|-|-|-|logistics 0-2|\n",
    "|02 Economics - 21 Wittayakit Building|-|-|-|71.1%|0.9 and 8|N|logistics 0-4|\n",
    "|02 Economics - 12 CU Terrace|-|-|-|68.1%|0.8 and 9|Y|logistics 0-4|\n",
    "\n",
    "Top 5 routes - SARIMAX\n",
    "\n",
    "|Depart - Destination Station|2018/2|SARIMAX|AIC|round|2019/1|SARIMAX|AIC|round|\n",
    "|----------------------------|------|-------|---|-----|------|-------|---|-----|\n",
    "|12 CU Terrace - 04 Engineering|69.1%|(2,0,2)x(2,0,2,14)|938.8|6|55.6%|(0,0,2)x(2,0,2,15)|837.0|6|\n",
    "|04 Engineering - 12 CU Terrace|70.2%|(0,0,0)x(1,0,2,15)|562.0|5|65.9%|(0,0,0)x(2,0,2,15)|439.5|5|\n",
    "|12 CU Terrace - 03 Sala Prakeaw|79.3%|(1,0,2)x(2,0,2,15)|528.5|4|82.2%|(0,0,1)x(2,0,2,15)|252.3|6|\n",
    "|04 Engineering - 22 Siam Square Soi 8|79.2%|(0,0,2)x(2,0,2,15)|532.7|4|73.3%|(0,0,0)x(1,0,2,15)|566.8|7|\n",
    "|04 Engineering - 21 Wittayakit Building|80.4%|(2,0,0)x(2,0,2,15)|466.4|4|-|-|-|-|\n",
    "|02 Economics - 21 Wittayakit Building|-|-|-|-|65.3%|(0,0,2)x(2,0,2,15)|544.2|7|\n",
    "|02 Economics - 12 CU Terrace|-|-|-|-|73.3%|(2,0,0)x(2,0,2,15)|559.1|3|\n",
    "\n",
    "Depart Station: 12 CU Terrace\n",
    "\n",
    "|Destination Station|2018/2|scale & rounding|upper|2019/1|scale & rounding|upper|Remark\n",
    "|-------------------|------|----------------|-----|------|----------------|-----|------\n",
    "|01 Exit to Cham square|79.2%|0.4 and 1|N|84.4%|0.8 and 7|Y|weird trend afterwards(both)|\n",
    "|02 Economics|85.3%|0.5 and 5|N|84.4%|0.2 and 6|N||\n",
    "|03 Sala Prakeaw|82.8%|0.1 and 3|N|77.8%|0.1 and 5|N||\n",
    "|04 Engineering|74.7%|0.6 and 0.37|N|56.1%|0.1 and 9|N||\n",
    "|05 Arts|94.5%|0.1 and 4|Y|78.7%|0.8 and 6|Y||\n",
    "|06 Chamchuri 9|97.0%|0.4 and 4|Y|60.0%|0.1 and 9|Y||\n",
    "|07 Chamchuri 5|100%|-|-|100%|-|-|underfit|\n",
    "|08 Witthaya Nives|100%|-|-|100%|-|-|underfit|\n",
    "|09 Chamchuri 10|90.3%|02 and 3|N|88.9%|0.2 and 8|N||\n",
    "|10 Chulaphat 14|100%|-|-|100%|-|-|underfit|\n",
    "|11 BTS-National stadium|88.3%|0.1 and 5|N|80%|0.1 and 3|N||\n",
    "|12 CU Terrace|93.9%|0.2 and 1|N|88.9%|0.9 and 3|N|no logistics|\n",
    "|13 Suan Luang sqaure|100%|-|-|100%|-|-|no data|\n",
    "|14 I'm Park|100%|-|-|100%|-|-|underfit|\n",
    "|15 U Center|100%|-|-|100%|-|-|underfit and weird trend, no logistics|\n",
    "|16 Communication Arts|81.2%|0.3 and 2|N|80.0%|0.1 and 4|N|underfit|\n",
    "|17 Property office|100%|-|-|100%|-|-|underfit|\n",
    "|18 Art and Culture|81.2%|0.8 and 2|N|86.7%|0.9 and 4|N|no logistics|\n",
    "|19 Pharmaceutical Science|100%|-|-|19.5%|-|-|2018/2 underfit, 2019/1 weird trend|\n",
    "|20 Veterinary Science|94.5%|0.2 and 2|N|71.1%|0.5 and 2|N|no logistics|\n",
    "|21 Wittayakit Building|85.5%|0.1 and 3|Y|91.1%|0.6 and 6|Y|no logistics|\n",
    "|22 Siam Square Soi 8|77.0%|0.1 and 4|Y|88.9%|0.9 and 8|Y||\n",
    "|average|86.4%|-|-|79.1%|-|-||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged_no_hol = pd.read_csv('no_hol_v2_edited.csv')\n",
    "# df_2018s2_old = pd.read_csv('2018_2_no_exam_v2.csv')\n",
    "# df_2018s2 = pd.read_csv('2018_2_new.csv')\n",
    "# df_2019s1 = pd.read_csv('2019_1.csv')\n",
    "# df_closed = pd.read_csv('closed_semester.csv')\n",
    "merged = pd.read_csv('merged.csv')\n",
    "# frames = [df_2018s2_old, df_closed, df_2019s1]\n",
    "# joined = pd.concat(frames)\n",
    "# # df = joined[joined['Station_depart.'] == '12 CU Terrace']\n",
    "# df = joined\n",
    "# weather_2018s2 = pd.read_csv('2018s2_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>regist_dt_ICT</th>\n",
       "      <th>start_dt_ICT</th>\n",
       "      <th>end_dt_ICT</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Station_depart.</th>\n",
       "      <th>Station_dest.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>7/1/2019</td>\n",
       "      <td>8:44:00</td>\n",
       "      <td>9:02:11</td>\n",
       "      <td>9:08:28</td>\n",
       "      <td>CP#0PFV4</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>7/1/2019</td>\n",
       "      <td>8:50:49</td>\n",
       "      <td>8:51:18</td>\n",
       "      <td>8:58:37</td>\n",
       "      <td>CP#0PVP6</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7/1/2019</td>\n",
       "      <td>11:05:15</td>\n",
       "      <td>11:06:27</td>\n",
       "      <td>11:11:10</td>\n",
       "      <td>CP#0PGZ3</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>7/1/2019</td>\n",
       "      <td>12:37:46</td>\n",
       "      <td>12:39:18</td>\n",
       "      <td>12:44:42</td>\n",
       "      <td>CP#0PVP6</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7/1/2019</td>\n",
       "      <td>13:39:56</td>\n",
       "      <td>13:40:42</td>\n",
       "      <td>13:45:58</td>\n",
       "      <td>CP#0S822</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>27/9/2019</td>\n",
       "      <td>12:27:03</td>\n",
       "      <td>12:43:14</td>\n",
       "      <td>12:50:36</td>\n",
       "      <td>CP#0VBN2</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5680</th>\n",
       "      <td>27/9/2019</td>\n",
       "      <td>16:16:45</td>\n",
       "      <td>16:22:32</td>\n",
       "      <td>16:27:29</td>\n",
       "      <td>CP#0SF89</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>30/9/2019</td>\n",
       "      <td>7:59:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CP#0SZM7</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>30/9/2019</td>\n",
       "      <td>8:32:13</td>\n",
       "      <td>8:32:46</td>\n",
       "      <td>8:38:59</td>\n",
       "      <td>CP#0V2Y4</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>30/9/2019</td>\n",
       "      <td>8:44:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CP#0SLD2</td>\n",
       "      <td>12 CU Terrace</td>\n",
       "      <td>04 Engineering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>431 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date regist_dt_ICT start_dt_ICT end_dt_ICT Customer_ID  \\\n",
       "52     7/1/2019       8:44:00      9:02:11    9:08:28    CP#0PFV4   \n",
       "53     7/1/2019       8:50:49      8:51:18    8:58:37    CP#0PVP6   \n",
       "54     7/1/2019      11:05:15     11:06:27   11:11:10    CP#0PGZ3   \n",
       "55     7/1/2019      12:37:46     12:39:18   12:44:42    CP#0PVP6   \n",
       "56     7/1/2019      13:39:56     13:40:42   13:45:58    CP#0S822   \n",
       "...         ...           ...          ...        ...         ...   \n",
       "5679  27/9/2019      12:27:03     12:43:14   12:50:36    CP#0VBN2   \n",
       "5680  27/9/2019      16:16:45     16:22:32   16:27:29    CP#0SF89   \n",
       "5788  30/9/2019       7:59:18          NaN        NaN    CP#0SZM7   \n",
       "5789  30/9/2019       8:32:13      8:32:46    8:38:59    CP#0V2Y4   \n",
       "5790  30/9/2019       8:44:38          NaN        NaN    CP#0SLD2   \n",
       "\n",
       "     Station_depart.   Station_dest.  \n",
       "52     12 CU Terrace  04 Engineering  \n",
       "53     12 CU Terrace  04 Engineering  \n",
       "54     12 CU Terrace  04 Engineering  \n",
       "55     12 CU Terrace  04 Engineering  \n",
       "56     12 CU Terrace  04 Engineering  \n",
       "...              ...             ...  \n",
       "5679   12 CU Terrace  04 Engineering  \n",
       "5680   12 CU Terrace  04 Engineering  \n",
       "5788   12 CU Terrace  04 Engineering  \n",
       "5789   12 CU Terrace  04 Engineering  \n",
       "5790   12 CU Terrace  04 Engineering  \n",
       "\n",
       "[431 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_o = df[df['Station_depart.'] == '12 CU Terrace']\n",
    "df_od = df_o[df_o['Station_dest.'] == '04 Engineering']\n",
    "\n",
    "df_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od['DateTime'] = df_od[df_od.columns[0:2]].apply(lambda x : '/' .join(x.astype(str)),axis=1)\n",
    "df_od['DateTime'] = df_od['DateTime'].str.replace(\":\", \"/\")\n",
    "\n",
    "\n",
    "demand = []\n",
    "demand = [1.0] * len(df_od)\n",
    "df_od['Demand'] = demand\n",
    "df_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_od['Date'] = pd.to_datetime(df_od['Date'], dayfirst=True)\n",
    "df_od.set_index('Date', inplace=True)\n",
    "# df_train = df_train_clone = df_od[:'2019-04-30']\n",
    "# df_test = df_test_clone = df_od['2019-05-01':'2019-05-23']\n",
    "df_train = df_train_clone = df_od[:'2019-03-29']\n",
    "df_test = df_test_clone = df_od['2019-04-01':'2019-05-03']\n",
    "df_train2 = df_train_clone2 = df_od['2019-08-13':'2019-09-20']\n",
    "df_test2 = df_test_clone2 = df_od['2019-09-23':'2019-09-27']\n",
    "# df_cv = df_cv_clone = df_od[:'2019-05-23']\n",
    "df_train_clone.reset_index(inplace=True)\n",
    "df_test_clone.reset_index(inplace=True)\n",
    "df_train_clone2.reset_index(inplace=True)\n",
    "df_test_clone2.reset_index(inplace=True)\n",
    "# df_cv_clone = df_cv_clone.reset_index(inplace=True)\n",
    "df_train_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 7\n",
    "stop = 21\n",
    "days_forecast = 15\n",
    "#14*15+45 or 18*15+60 (train = :2019-03-29) or  for 2018/2, 6 for 2019/1\n",
    "hours_multiplier = 15\n",
    "prediction_size = days_forecast*hours_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_od.reset_index(inplace=True)\n",
    "df_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df, df_main, type):\n",
    "    date = list(dict.fromkeys(df_main['Date']))\n",
    "    if (type == True):\n",
    "        oper_time = list(np.arange(start, stop, 0.5))\n",
    "    else:\n",
    "        oper_time = list(np.arange(start, stop))\n",
    "    timestamp_all = []\n",
    "\n",
    "    for each in date:\n",
    "        each = str(each)\n",
    "        each = each[0:11]\n",
    "        for h in oper_time:\n",
    "            (y, m, d) = each.split('-')\n",
    "            d, m, y, hh = int(d), int(m), int(y), int(h)\n",
    "            if (((h*10)%10) == 5):\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh, minute=30)\n",
    "            else:\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "            timestamp_all.append(timestamp)\n",
    "\n",
    "    timestamp_fill = list(set(timestamp_all) - set((list(dict.fromkeys(df_main['Timestamp'])))))\n",
    "    \n",
    "    demand_fill = []\n",
    "    demand_fill = [0.0] * len(timestamp_fill)\n",
    "\n",
    "    data_fill = {'Timestamp': timestamp_fill, 'Demand': demand_fill}    \n",
    "    df_od_fill = pd.DataFrame(data_fill)\n",
    "    df_od_fill.sort_values('Timestamp', inplace=True)\n",
    "    df_od_fill.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df, backup = group_by_time(df)\n",
    "    \n",
    "    df = df.append(df_od_fill)\n",
    "    df.sort_values('Timestamp', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def group_by_time(df):\n",
    "    df = df.groupby('Timestamp').sum()\n",
    "    backup = df\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    return df, backup\n",
    "\n",
    "def create_window(df, type):\n",
    "    converted_regist = []\n",
    "\n",
    "    for slot in df['regist_dt_ICT']:\n",
    "        (h, m, s) = slot.split(':')\n",
    "        if (type == True):\n",
    "            if (int(m) >= 30):\n",
    "                time = int(h) + 0.5\n",
    "            else:\n",
    "                time = int(h)\n",
    "        else:\n",
    "            time = int(h)\n",
    "        converted_regist.append(time)    \n",
    "\n",
    "    df['Converted_Regist'] = converted_regist\n",
    "\n",
    "    df = df[df['Converted_Regist'] >= start]\n",
    "    df = df[df['Converted_Regist'] < stop]\n",
    "    df = df.drop(['Converted_Regist'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_timestamp(df, type):\n",
    "    timestamp_converted = []\n",
    "    for slot in df['DateTime']:\n",
    "        (d, m, y, hh, mm, ss) = slot.split('/')\n",
    "        d, m, y, hh, mm = int(d), int(m), int(y), int(hh), int(mm)\n",
    "        if (type == True):\n",
    "            if (mm >= 30):\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh, minute=30)\n",
    "            else:\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "        else:\n",
    "            timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "        timestamp_converted.append(timestamp)\n",
    "    df['Timestamp'] = timestamp_converted\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timestamp(df, type):\n",
    "    timestamp_converted = []\n",
    "    for slot in df['DateTime']:\n",
    "        (d, m, y, hh, mm, ss) = slot.split('/')\n",
    "        d, m, y, hh, mm = int(d), int(m), int(y), int(hh), int(mm)\n",
    "        if (type == True):\n",
    "            if (mm >= 30):\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh, minute=30)\n",
    "            else:\n",
    "                timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "        else:\n",
    "            timestamp = pd.Timestamp(year=y, month=m, day=d, hour=hh)\n",
    "        timestamp_converted.append(timestamp)\n",
    "    df['Timestamp'] = timestamp_converted\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert24(df):\n",
    "    time24 = []\n",
    "    for each in df['Time']:\n",
    "        if each[-2:] == 'AM':\n",
    "            new = each[:-3]+':00'\n",
    "        elif each[-2:] == 'PM' and each[:2] == '12':\n",
    "            new = each[:-3]+':00'\n",
    "        else:\n",
    "            new = str(int(each[:each.find(':')])+12)+each[each.find(':'):-3]+':00'\n",
    "        time24.append(new)\n",
    "    df['Time'] = time24\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_num(df):\n",
    "    temp = []\n",
    "    humidity = []\n",
    "    windspeed = []\n",
    "    pressure = []\n",
    "    \n",
    "    for t,h,w,p in zip(df['Temperature'], df['Humidity'], df['Windspeed'], df['Pressure']):\n",
    "        tt = float(t[0:2])\n",
    "        hh = float(h[0:2])\n",
    "        ww = float(w[0:2])\n",
    "        pp = float(p[0:5])\n",
    "        temp.append(tt)\n",
    "        humidity.append(hh)\n",
    "        windspeed.append(ww)\n",
    "        pressure.append(pp)\n",
    "    \n",
    "    df['Temperature'] = temp\n",
    "    df['Humidity'] = humidity\n",
    "    df['Windspeed'] = windspeed\n",
    "    df['Pressure'] = pressure\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2018s2 = convert24(weather_2018s2)\n",
    "\n",
    "weather_2018s2['DateTime'] = weather_2018s2[weather_2018s2.columns[0:2]].apply(lambda x : '/' .join(x.astype(str)),axis=1)\n",
    "weather_2018s2['DateTime'] = weather_2018s2['DateTime'].str.replace(\":\", \"/\")\n",
    "\n",
    "weather_2018s2 = create_timestamp(weather_2018s2, True)\n",
    "weather_2018s2 = extract_num(weather_2018s2)\n",
    "weather_2018s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clone = create_timestamp(df_train_clone, False)\n",
    "train_framed = create_window(df_train_clone, False)\n",
    "train, backup_train = group_by_time(train_framed)\n",
    "train = train_clone = fill_missing(train, df_train_clone, False)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged = train.join(weather_2018s2.set_index('Timestamp'), on='Timestamp')\n",
    "train_merged = train_merged[['Timestamp', 'Demand', 'Temperature', 'Humidity', 'Windspeed', 'Pressure']]\n",
    "train_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_regressor = train_merged[['Timestamp', 'Temperature', 'Humidity', 'Windspeed', 'Pressure']]\n",
    "# train_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_clone = create_timestamp(df_test_clone, False)\n",
    "test_framed = create_window(df_test_clone, False)\n",
    "test, backup_test = group_by_time(test_framed)\n",
    "test = test_clone = fill_missing(test, df_test_clone, False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged = test.join(weather_2018s2.set_index('Timestamp'), on='Timestamp')\n",
    "test_merged = test_merged[['Timestamp', 'Demand', 'Temperature', 'Humidity', 'Windspeed', 'Pressure']]\n",
    "test_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_future = test_merged.iloc[0:140]\n",
    "for_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_regressor = test_merged[['Temperature', 'Humidity', 'Windspeed', 'Pressure']]\n",
    "# test_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clone2 = create_timestamp(df_train_clone2, False)\n",
    "train_framed2 = create_window(df_train_clone2, False)\n",
    "train2, backup_train2 = group_by_time(train_framed2)\n",
    "train2 = train_clone2 = fill_missing(train2, df_train_clone2, False)\n",
    "train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_clone2 = create_timestamp(df_test_clone2, False)\n",
    "test_framed2= create_window(df_test_clone2, False)\n",
    "test2, backup_test2 = group_by_time(test_framed2)\n",
    "test2 = test_clone2 = fill_missing(test2, df_test_clone2, False)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv, backup_cv = group_by_time(df_cv)\n",
    "# cv = fill_missing(cv, df_cv_clone)\n",
    "# cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prophet = cv[['Timestamp', 'Demand']]\n",
    "# df_prophet = df_prophet.rename(columns={'Timestamp': 'ds', 'Demand': 'y'})\n",
    "# df_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clone.set_index('Timestamp', inplace=True)\n",
    "test_clone.set_index('Timestamp', inplace=True)\n",
    "train_clone2.set_index('Timestamp', inplace=True)\n",
    "test_clone2.set_index('Timestamp', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(211)\n",
    "plt.plot(train_clone)\n",
    "plt.plot(test_clone)\n",
    "plt.subplot(212)\n",
    "plt.plot(train_clone2)\n",
    "plt.plot(test_clone2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Day of week</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Week of year</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>is_lunch_break</th>\n",
       "      <th>is_class_time</th>\n",
       "      <th>1204</th>\n",
       "      <th>412</th>\n",
       "      <th>1203</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>7:00</td>\n",
       "      <td>2019-01-07 07:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.89</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.90</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>8:00</td>\n",
       "      <td>2019-01-07 08:00</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.93</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>9:00</td>\n",
       "      <td>2019-01-07 09:00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.93</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>10:00</td>\n",
       "      <td>2019-01-07 10:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.79</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29.96</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>11:00</td>\n",
       "      <td>2019-01-07 11:00</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>12.0</td>\n",
       "      <td>29.93</td>\n",
       "      <td>NO</td>\n",
       "      <td>YES</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>16:00</td>\n",
       "      <td>2019-03-29 16:00</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.53</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.70</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>17:00</td>\n",
       "      <td>2019-03-29 17:00</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.70</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>18:00</td>\n",
       "      <td>2019-03-29 18:00</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.70</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>19:00</td>\n",
       "      <td>2019-03-29 19:00</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>13.0</td>\n",
       "      <td>29.73</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>2019-03-29</td>\n",
       "      <td>20:00</td>\n",
       "      <td>2019-03-29 20:00</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.74</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.76</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   Time         Timestamp  Day of week  Hour  Quarter  Month  \\\n",
       "0    2019-01-07   7:00  2019-01-07 07:00            1     7        1      1   \n",
       "1    2019-01-07   8:00  2019-01-07 08:00            1     8        1      1   \n",
       "2    2019-01-07   9:00  2019-01-07 09:00            1     9        1      1   \n",
       "3    2019-01-07  10:00  2019-01-07 10:00            1    10        1      1   \n",
       "4    2019-01-07  11:00  2019-01-07 11:00            1    11        1      1   \n",
       "..          ...    ...               ...          ...   ...      ...    ...   \n",
       "751  2019-03-29  16:00  2019-03-29 16:00            5    16        1      3   \n",
       "752  2019-03-29  17:00  2019-03-29 17:00            5    17        1      3   \n",
       "753  2019-03-29  18:00  2019-03-29 18:00            5    18        1      3   \n",
       "754  2019-03-29  19:00  2019-03-29 19:00            5    19        1      3   \n",
       "755  2019-03-29  20:00  2019-03-29 20:00            5    20        1      3   \n",
       "\n",
       "     Day of year  Day of month  Week of year  ...  Humidity Wind Speed  \\\n",
       "0              7             7             2  ...      0.89        3.0   \n",
       "1              7             7             2  ...      0.83        5.0   \n",
       "2              7             7             2  ...      0.83        5.0   \n",
       "3              7             7             2  ...      0.79        7.0   \n",
       "4              7             7             2  ...      0.70       12.0   \n",
       "..           ...           ...           ...  ...       ...        ...   \n",
       "751           88            29            13  ...      0.53       10.0   \n",
       "752           88            29            13  ...      0.53        9.0   \n",
       "753           88            29            13  ...      0.59       10.0   \n",
       "754           88            29            13  ...      0.70       13.0   \n",
       "755           88            29            13  ...      0.74       10.0   \n",
       "\n",
       "     Pressure  is_lunch_break  is_class_time 1204 412  1203  421  422  \n",
       "0       29.90              NO             NO    0   0     1    0    0  \n",
       "1       29.93              NO             NO    2   0     0    1    0  \n",
       "2       29.93              NO            YES    0   1     0    0    0  \n",
       "3       29.96              NO            YES    0   0     0    0    0  \n",
       "4       29.93              NO            YES    1   0     0    0    0  \n",
       "..        ...             ...            ...  ...  ..   ...  ...  ...  \n",
       "751     29.70              NO             NO    0   1     0    0    0  \n",
       "752     29.70              NO             NO    0   0     0    1    1  \n",
       "753     29.70              NO             NO    0   0     0    0    1  \n",
       "754     29.73              NO             NO    0   0     0    0    0  \n",
       "755     29.76              NO             NO    0   0     0    0    0  \n",
       "\n",
       "[756 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new = df_2018s2.iloc[0:756]\n",
    "train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>412</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>2019-04-01 07:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>2019-04-01 08:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>2019-04-01 09:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>2019-04-01 10:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2019-04-01 11:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>2019-05-03 16:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>2019-05-03 17:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>2019-05-03 18:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>2019-05-03 19:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>2019-05-03 20:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp  412\n",
       "756   2019-04-01 07:00    0\n",
       "757   2019-04-01 08:00    0\n",
       "758   2019-04-01 09:00    0\n",
       "759   2019-04-01 10:00    0\n",
       "760   2019-04-01 11:00    2\n",
       "...                ...  ...\n",
       "1045  2019-05-03 16:00    1\n",
       "1046  2019-05-03 17:00    0\n",
       "1047  2019-05-03 18:00    0\n",
       "1048  2019-05-03 19:00    0\n",
       "1049  2019-05-03 20:00    0\n",
       "\n",
       "[294 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new = df_2018s2.iloc[756:]\n",
    "test_new = test_new[['Timestamp', '412']]\n",
    "test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prophet = train_new[['Timestamp', '412']]\n",
    "df_prophet.rename(columns={'Timestamp': 'ds', '412': 'y'}, inplace=True)\n",
    "df_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "df_prophet['cap'] = 4\n",
    "df_prophet['floor'] = 0\n",
    "m = Prophet(growth='logistic', changepoint_prior_scale=0.4)\n",
    "# m = Prophet(changepoint_prior_scale=0.5)\n",
    "# m.add_regressor('Temperature')\n",
    "# m.add_regressor('Humidity')\n",
    "# m.add_regressor('Windspeed')\n",
    "# m.add_regressor('Pressure')\n",
    "m.fit(df_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=840, freq='H')\n",
    "future['cap'] = 4\n",
    "future['floor'] = 0\n",
    "future = future[(future['ds'].dt.hour >= start) & (future['ds'].dt.hour < stop)]\n",
    "future = future[future['ds'].dt.dayofweek < 5]\n",
    "future.reset_index(drop=True, inplace=True)\n",
    "# future['Temperature'] = for_future[['Temperature']]\n",
    "# future['Humidity'] = for_future['Humidity']\n",
    "# future['Windspeed'] = for_future['Windspeed']\n",
    "# future['Pressure'] = for_future['Pressure']\n",
    "forecast = m.predict(future)\n",
    "yhat_round = []\n",
    "for each in forecast['yhat']:\n",
    "    if(each < 0):\n",
    "        each = 0\n",
    "    elif ((each*10)%10 >= 4):\n",
    "        each = math.ceil(each)\n",
    "    else:\n",
    "        each = math.floor(each)\n",
    "#     each = math.ceil(each)\n",
    "    yhat_round.append(each)\n",
    "forecast['yhat_nr'] = forecast['yhat']\n",
    "forecast['yhat'] = yhat_round\n",
    "forecast[['ds', 'yhat', 'yhat_nr', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_eva(train, forecast, prediction_size):\n",
    "    forecast = forecast[:prediction_size]\n",
    "    \n",
    "    return forecast.set_index('ds')[['yhat', 'yhat_nr', 'yhat_lower', 'yhat_upper']].join(train.set_index('ds'))\n",
    "\n",
    "def cal_SMAPE(df):\n",
    "    print(df)\n",
    "#     smape = 100/len(df) * np.sum(2 * np.abs(df['yhat'] - df['y']) / (np.abs(df['y']) + np.abs(df['yhat'])))\n",
    "    smape = np.sum(np.abs(df['yhat'] - df['y']))/np.sum(df['y'] + df['yhat'])\n",
    "    return 'sMAPE', smape, 'accuracy', 100-100*smape\n",
    "\n",
    "def cal_metrics(df):\n",
    "    print(df)\n",
    "    r2 = r2_score(df['y'], df['yhat'])\n",
    "    rmse = np.sqrt(mean_squared_error(df['y'], df['yhat']))\n",
    "    mae = mean_absolute_error(df['y'], df['yhat'])\n",
    "    \n",
    "    return rmse, mae, mae_p, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = join_eva(df_prophet, forecast, len(df_prophet))\n",
    "\n",
    "print(cal_metrics(train), cal_SMAPE(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_test(test, forecast, prediction_size):\n",
    "    df_test = test.rename(columns={'Timestamp': 'ds', '412': 'y'})\n",
    "    forecast = forecast[len(df_prophet):len(df_prophet)+prediction_size+125]\n",
    "    joined = forecast.set_index('ds')[['yhat', 'yhat_nr', 'yhat_lower', 'yhat_upper']].join(df_test.set_index('ds'))\n",
    "    \n",
    "    return joined.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eva_df = join_test(test_new, forecast, prediction_size)\n",
    "\n",
    "print(cal_metrics(eva_df), cal_SMAPE(eva_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASE(training_series, testing_series, prediction_series):\n",
    "    \"\"\"\n",
    "    Computes the MEAN-ABSOLUTE SCALED ERROR forcast error for univariate time series prediction.\n",
    "    \n",
    "    See \"Another look at measures of forecast accuracy\", Rob J Hyndman\n",
    "    \n",
    "    parameters:\n",
    "        training_series: the series used to train the model, 1d numpy array\n",
    "        testing_series: the test series to predict, 1d numpy array or float\n",
    "        prediction_series: the prediction of testing_series, 1d numpy array (same size as testing_series) or float\n",
    "        absolute: \"squares\" to use sum of squares and root the result, \"absolute\" to use absolute values.\n",
    "    \n",
    "    \"\"\"\n",
    "#     print \"Needs to be tested.\"\n",
    "    print(training_series, testing_series, prediction_series)\n",
    "    n = training_series.shape[0]\n",
    "    d = np.abs(  np.diff( training_series) ).sum()/(n-1)\n",
    "    \n",
    "    errors = np.abs(testing_series - prediction_series )\n",
    "    return errors.mean()/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MASE(df_prophet['y'], test['Demand'], forecast['yhat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eva_df['yhat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scales = np.arange(0.1,1.0,0.1)\n",
    "rounding_boundary = np.arange(1, 10, 1)\n",
    "\n",
    "def grid_search():\n",
    "    scale = [] \n",
    "    round_at = []\n",
    "    eva_acc = []\n",
    "    test_acc = []\n",
    "    remark = []\n",
    "    for param in scales:\n",
    "        for boundary in rounding_boundary:\n",
    "            scale.append(param)\n",
    "            round_at.append(boundary)\n",
    "            df_prophet['cap'] = 4\n",
    "            df_prophet['floor'] = 0\n",
    "            m = Prophet(growth='logistic', changepoint_prior_scale=param)\n",
    "#             m = Prophet(changepoint_prior_scale=param) \n",
    "            m.fit(df_prophet)\n",
    "            future = m.make_future_dataframe(periods=840, freq='H')\n",
    "            future['cap'] = 4\n",
    "            future['floor'] = 0\n",
    "            future = future[(future['ds'].dt.hour >= start) & (future['ds'].dt.hour < stop)]\n",
    "            future = future[future['ds'].dt.dayofweek < 5]\n",
    "            forecast = m.predict(future)\n",
    "            yhat_round = []            \n",
    "            for each in forecast['yhat']:\n",
    "                if (each < 0):\n",
    "                    each = 0\n",
    "                elif (((each*10)%10) >= boundary):\n",
    "                    each = math.ceil(each)\n",
    "                else:\n",
    "                    each = math.floor(each)\n",
    "                yhat_round.append(each)\n",
    "                \n",
    "            forecast['yhat_nr'] = forecast['yhat']\n",
    "            forecast['yhat'] = yhat_round\n",
    "\n",
    "            train = join_eva(df_prophet, forecast, len(df_prophet))\n",
    "            acc = cal_SMAPE(train)\n",
    "            eva_acc.append(acc)\n",
    "\n",
    "            eva_df = join_test(test_new, forecast, prediction_size)\n",
    "            acc = cal_SMAPE(eva_df)\n",
    "            test_acc.append(acc)\n",
    "            \n",
    "            zero_counter = 0\n",
    "            for each in eva_df['yhat']:\n",
    "                if(each == 0):\n",
    "                    zero_counter += 1\n",
    "            \n",
    "            if(zero_counter == len(eva_df)):\n",
    "                remark.append('all 0')\n",
    "            else:\n",
    "                remark.append('usable')\n",
    "    \n",
    "    return scale, round_at, eva_acc, test_acc, remark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale, round_at, eva_acc, test_acc, remark = grid_search()\n",
    "grid = {'Scale': scale, 'Round at': round_at, 'Fit': eva_acc, 'Test': test_acc, 'Remark': remark}    \n",
    "df_grid = pd.DataFrame(grid)\n",
    "df_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_grid_sorted = df_grid.query('Remark == \"usable\"')\n",
    "df_grid_sorted = df_grid_sorted.sort_values(['Test'])\n",
    "df_grid_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid_sorted_uncut = df_grid.sort_values(['Test'])\n",
    "df_grid_sorted_uncut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation\n",
    "df_cv = cross_validation(m, initial = '90 days', horizon = '20 days')\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import performance_metrics\n",
    "df_p = performance_metrics(df_cv)\n",
    "df_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import product\n",
    "import statsmodels as sm\n",
    "import statsmodels.api as sm_api\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pylab import rcParams\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arima = create_timestamp(df_train_clone, False)\n",
    "train_arima = fill_missing(train_arima, df_train_clone, False)\n",
    "train_arima.set_index(['Timestamp'], inplace=True)\n",
    "train_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arima = create_timestamp(df_test_clone, False)\n",
    "test_arima = fill_missing(test_arima, df_test_clone, False)\n",
    "test_arima.set_index(['Timestamp'], inplace=True)\n",
    "test_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endo = train_new[['Timestamp', '1204']]\n",
    "endo.set_index('Timestamp', inplace=True)\n",
    "endo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo = train_merged[['Timestamp', 'Temperature', 'Humidity', 'Pressure', 'Windspeed']]\n",
    "exo.set_index('Timestamp', inplace=True)\n",
    "exo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_test = test_merged[['Timestamp', 'Temperature', 'Humidity', 'Pressure', 'Windspeed']]\n",
    "exo_test.set_index('Timestamp', inplace=True)\n",
    "exo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=15).mean()\n",
    "    rolstd = timeseries.rolling(window=15).std()\n",
    "    #Plot rolling statistics:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "\n",
    "test_stationarity(endo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_plot(y, lags=None, figsize=(14,7), style='bmh'):\n",
    "#     if not isinstance(y, pd.Series):\n",
    "#         y = pd.Series(y)\n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('TSA-DF: p={0:5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "ts_plot(endo, lags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(endo, period=14)\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(411)\n",
    "plt.plot(endo, label='Original')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonality')\n",
    "plt.legend(loc='upper right')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = range(0, 3)\n",
    "d = 0\n",
    "q = range(0, 3)\n",
    "s = 75\n",
    "pdq = []\n",
    "seasonal_pdq = []\n",
    "for ar in p:\n",
    "    for ma in q:\n",
    "        param = (ar, d, ma)\n",
    "        sparam = (ar, d, ma, s)\n",
    "        pdq.append(param)\n",
    "        seasonal_pdq.append(sparam)\n",
    "\n",
    "# pdq = list(itertools.product(p, d, q))\n",
    "# seasonal_pdq = [(x[0], x[1], x[2], 75) for x in list(itertools.product(p, q))]\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm_api.tsa.statespace.SARIMAX(train_arima, order=param, seasonal_order=param_seasonal, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('SARIMAX{}x{}14 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm_api.tsa.statespace.SARIMAX(endog=endo, exog=exo,\n",
    "                                order=(1, 0, 2),\n",
    "                                seasonal_order=(2, 0, 2, 15),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "print(results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start=pd.to_datetime('2019-03-01 07:00:00'), dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "\n",
    "ax = endo['2019-01-07 07:00:00':].plot(label='observed', figsize=(14, 7))\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Demand Hourly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime('2019-03-01 07:00:00'), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "\n",
    "ax = train_arima['2019-01-07 07:00:00':].plot(label='observed', figsize=(14, 7))\n",
    "\n",
    "for each in pred_dynamic.predicted_mean:\n",
    "    if (each < 0):\n",
    "        pred_dynamic.predicted_mean = pred_dynamic.predicted_mean.replace(each, 0)\n",
    "    elif (((each*10)%10) > 3):\n",
    "        pred_dynamic.predicted_mean = pred_dynamic.predicted_mean.replace(each, math.ceil(each))\n",
    "    else:\n",
    "        pred_dynamic.predicted_mean = pred_dynamic.predicted_mean.replace(each, math.floor(each))\n",
    "        \n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2019-04-01'), train_arima.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Demand Hourly')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_uc = results.get_forecast(steps=70, exog=for_future[['Temperature', 'Humidity', 'Windspeed', 'Pressure']])\n",
    "pred_uc = results.get_forecast(steps=70)\n",
    "pred_ci = pred_uc.conf_int()\n",
    "\n",
    "plt.plot(pred_uc.predicted_mean)\n",
    "print(pred_uc.predicted_mean, pred_ci.iloc[:, 1])\n",
    "# ax = train_arima.plot(label='observed', figsize=(20, 15))\n",
    "# pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "# ax.fill_between(pred_ci.index,\n",
    "#                 pred_ci.iloc[:, 0],\n",
    "#                 pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "# ax.set_xlabel('Date')\n",
    "# ax.set_ylabel('Demand Hourly')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "y_forecasted = pred_uc.predicted_mean\n",
    "for each in y_forecasted:\n",
    "    if (each < 0):\n",
    "        each = 0\n",
    "    elif (((each*10)%10) > 6):\n",
    "        each = math.ceil(each)\n",
    "    else:\n",
    "        each = math.floor(each)\n",
    "    \n",
    "    new_list.append(each)\n",
    "\n",
    "for_future['predict_n'] = new_list\n",
    "for_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_SMAPE(df, type):\n",
    "    if type:\n",
    "        smape = 100/len(df) * np.sum(2 * np.abs(df['Demand'] - df['predict_u']) / (np.abs(df['Demand']) + np.abs(df['predict_u'])))\n",
    "    else:\n",
    "        smape = 100/len(df) * np.sum(2 * np.abs(df['Demand'] - df['predict_n']) / (np.abs(df['Demand']) + np.abs(df['predict_n'])))\n",
    "    print(df)\n",
    "    return 'sMAPE', smape, 'accuracy', 100-smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_SMAPE(for_future, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounding_boundary_sarimax = np.arange(1, 10, 1)\n",
    "\n",
    "def grid_search_sarimax(df, pred):\n",
    "    pred_n = pred\n",
    "    pred_n = pred_n.predicted_mean\n",
    "    pred_ci = pred.conf_int()\n",
    "    pred_u = pred_ci.iloc[:, 1]\n",
    " \n",
    "    round_at = []\n",
    "    test_acc_n = []\n",
    "    test_acc_u = []\n",
    "    remark = []\n",
    "    \n",
    "    for boundary in rounding_boundary_sarimax:\n",
    "        n_list = []\n",
    "        u_list = []\n",
    "        round_at.append(boundary)\n",
    "        \n",
    "        for each in pred_n:\n",
    "            if (each < 0):\n",
    "                n = 0\n",
    "            elif (((each*10)%10) > boundary):\n",
    "                n = math.ceil(each)\n",
    "            else:\n",
    "                n = math.floor(each)\n",
    "\n",
    "            n_list.append(n)\n",
    "            \n",
    "        df['predict_n'] = n_list\n",
    "        acc_n = cal_SMAPE(df, False)\n",
    "        test_acc_n.append(acc_n)\n",
    "        zero_counter_n = 0\n",
    "        \n",
    "        for each in n_list:\n",
    "            if(each == 0):\n",
    "                zero_counter_n += 1\n",
    "                \n",
    "#         for each in pred_u:\n",
    "#             if (each < 0):\n",
    "#                 u = 0\n",
    "#             elif (((each*10)%10) > boundary):\n",
    "#                 u = math.ceil(each)\n",
    "#             else:\n",
    "#                 u = math.floor(each)\n",
    "\n",
    "#             u_list.append(u)\n",
    "            \n",
    "#         df['predict_u'] = u_list        \n",
    "#         acc_u = cal_SMAPE(df, True)\n",
    "#         test_acc_u.append(acc_u)\n",
    "        zero_counter_u = 0\n",
    "        \n",
    "#         for each in u_list:\n",
    "#             if(each == 0):\n",
    "#                 zero_counter_u += 1\n",
    "\n",
    "        if(zero_counter_n == zero_counter_u == len(df)):\n",
    "            remark.append('all 0')\n",
    "        elif(zero_counter_n == len(df)):\n",
    "            remark.append('use upper')\n",
    "        else:\n",
    "            remark.append('use normal')\n",
    "\n",
    "#     return round_at, test_acc_n, test_acc_u, remark\n",
    "    return round_at, test_acc_n, remark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_at, test_acc_n, test_acc_u, remark = grid_search_sarimax(test_arima, pred_uc)\n",
    "grid = {'Round at': round_at, 'Test': test_acc_n, 'Test(upper)': test_acc_u, 'Remark': remark}    \n",
    "df_grid = pd.DataFrame(grid)\n",
    "df_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_at, test_acc_n, remark = grid_search_sarimax(test_arima, pred_uc)\n",
    "grid = {'Round at': round_at, 'Test': test_acc_n, 'Remark': remark}    \n",
    "df_grid = pd.DataFrame(grid)\n",
    "df_grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
